{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a996d864-6a89-4513-95ba-2edb457d25be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Identification de langue avec un modèle de Deep Learning et**\n",
    "## **une couche d'embedding en input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb14523-04a8-424c-976a-d61585c0bbf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import joblib\n",
    "import pickle\n",
    "import json\n",
    "import keras\n",
    "import csv\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# Choix de la Tokenisation (1 = Keras, 2 = BERT, 3 = Tiktoken)\n",
    "sel_tokenization = 3\n",
    "\n",
    "## Pour résoudre les problème de mémoire et de performances\n",
    "max_length = 250\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bad4ba-8678-41a4-9008-ab1915eddae6",
   "metadata": {},
   "source": [
    "#### **Lectures des phrases et de leur étiquette \"Langue\" pour les langues sélectionnées**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "491d66b2-90bd-49e9-99cb-9601edf52a57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes de sentence.csv: 10345978\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ber</td>\n",
       "      <td>Yeqber uɛebbuḍ-iw seg wayen ččiɣ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ita</td>\n",
       "      <td>Sono venute alla moschea.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heb</td>\n",
       "      <td>אני לא יכול לפענח מה קרה.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rus</td>\n",
       "      <td>У меня очень много дел.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ita</td>\n",
       "      <td>Andiamo a imparare qualcosa in Uganda.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345973</th>\n",
       "      <td>deu</td>\n",
       "      <td>Wir werden das Problem nicht aufgreifen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345974</th>\n",
       "      <td>fra</td>\n",
       "      <td>Je suis cuit !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345975</th>\n",
       "      <td>epo</td>\n",
       "      <td>Atomenergio estas sekura.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345976</th>\n",
       "      <td>tok</td>\n",
       "      <td>o pana ala e moku tawa soweli tomo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345977</th>\n",
       "      <td>hun</td>\n",
       "      <td>Máris unod magad?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10345978 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lan_code                                  sentence\n",
       "0             ber         Yeqber uɛebbuḍ-iw seg wayen ččiɣ.\n",
       "1             ita                 Sono venute alla moschea.\n",
       "2             heb                 אני לא יכול לפענח מה קרה.\n",
       "3             rus                   У меня очень много дел.\n",
       "4             ita    Andiamo a imparare qualcosa in Uganda.\n",
       "...           ...                                       ...\n",
       "10345973      deu  Wir werden das Problem nicht aufgreifen.\n",
       "10345974      fra                            Je suis cuit !\n",
       "10345975      epo                 Atomenergio estas sekura.\n",
       "10345976      tok       o pana ala e moku tawa soweli tomo.\n",
       "10345977      hun                         Máris unod magad?\n",
       "\n",
       "[10345978 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ouvrir le fichier d'entrée en mode lecture\n",
    "def create_lang_df(path):\n",
    "    df = pd.read_csv(path, index_col ='id')\n",
    "    return df\n",
    "\n",
    "def save_list_lan(lan_code):\n",
    "    with open('../data/multilingue/lan_code.csv', 'w', newline='') as fichier_csv:\n",
    "        writer = csv.writer(fichier_csv)\n",
    "        writer.writerow(lan_code)\n",
    "    \n",
    "df = create_lang_df('../data/multilingue/sentences-big.csv')\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "with open('../data/multilingue/lan_to_language.json', 'r') as fichier:\n",
    "    lan_to_language = json.load(fichier)\n",
    "n_rows = len(df)\n",
    "print('Nombre de lignes de sentence.csv:',n_rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71dc774-4916-4658-b015-01594a8d003b",
   "metadata": {},
   "source": [
    "#### **Réalisation d'un jeu de données d'entrainement et de test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbac26f-2f26-4e3e-9685-1aa5914569b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deu</td>\n",
       "      <td>Wie die Erde entstanden ist, das ist eine Frage, die sich allen stellt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deu</td>\n",
       "      <td>Drinnen ist etwas Lebendiges.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rus</td>\n",
       "      <td>Я не хотела замуж.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kab</td>\n",
       "      <td>Ssarden-ak-t.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>por</td>\n",
       "      <td>Estou piscando para ele, mas ele não está olhando.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9828674</th>\n",
       "      <td>tat</td>\n",
       "      <td>Йөрәген бүләк иткәннәр, кан әйләнешендә кыенлыклар барлыкка килсә, гаҗәпләнергә тиеш түгел.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9828675</th>\n",
       "      <td>ber</td>\n",
       "      <td>Cmumeḥ-d yid-i ma ulac aɣilif.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9828676</th>\n",
       "      <td>kab</td>\n",
       "      <td>Ceɛɛel-as ɣer Tasga Mellul.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9828677</th>\n",
       "      <td>eng</td>\n",
       "      <td>Tom can't stand vegetables.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9828678</th>\n",
       "      <td>heb</td>\n",
       "      <td>היא עושה מדיטציה.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9757778 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lan_code  \\\n",
       "0            deu   \n",
       "1            deu   \n",
       "2            rus   \n",
       "3            kab   \n",
       "4            por   \n",
       "...          ...   \n",
       "9828674      tat   \n",
       "9828675      ber   \n",
       "9828676      kab   \n",
       "9828677      eng   \n",
       "9828678      heb   \n",
       "\n",
       "                                                                                            sentence  \n",
       "0                            Wie die Erde entstanden ist, das ist eine Frage, die sich allen stellt.  \n",
       "1                                                                      Drinnen ist etwas Lebendiges.  \n",
       "2                                                                                 Я не хотела замуж.  \n",
       "3                                                                                      Ssarden-ak-t.  \n",
       "4                                                 Estou piscando para ele, mas ele não está olhando.  \n",
       "...                                                                                              ...  \n",
       "9828674  Йөрәген бүләк иткәннәр, кан әйләнешендә кыенлыклар барлыкка килсә, гаҗәпләнергә тиеш түгел.  \n",
       "9828675                                                               Cmumeḥ-d yid-i ma ulac aɣilif.  \n",
       "9828676                                                                  Ceɛɛel-as ɣer Tasga Mellul.  \n",
       "9828677                                                                  Tom can't stand vegetables.  \n",
       "9828678                                                                            היא עושה מדיטציה.  \n",
       "\n",
       "[9757778 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de langues à classer: 95\n",
      "Nombre de lignes par langue:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_phrases_lang</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lan_code</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>afr</th>\n",
       "      <td>4137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ara</th>\n",
       "      <td>38651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arq</th>\n",
       "      <td>2336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asm</th>\n",
       "      <td>3205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avk</th>\n",
       "      <td>4102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>war</th>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wuu</th>\n",
       "      <td>4757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yid</th>\n",
       "      <td>9632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yue</th>\n",
       "      <td>6230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsm</th>\n",
       "      <td>6610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          nb_phrases_lang\n",
       "lan_code                 \n",
       "afr                  4137\n",
       "ara                 38651\n",
       "arq                  2336\n",
       "asm                  3205\n",
       "avk                  4102\n",
       "...                   ...\n",
       "war                  2025\n",
       "wuu                  4757\n",
       "yid                  9632\n",
       "yue                  6230\n",
       "zsm                  6610\n",
       "\n",
       "[95 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# créer 2 dataframes: 1 train (95% des phrases) et 1 test (5% des phrases)\n",
    "n_train = int(n_rows*0.95)\n",
    "df_train = df.iloc[:n_train].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_test = df.iloc[n_train:].sample(frac=1, random_state=24).reset_index(drop=True)\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "df_lan = pd.DataFrame(data= df.groupby('lan_code').size(), columns = ['nb_phrases_lang'] )\n",
    "\n",
    "# Filtrage des langues qui ont peu de phrases (>2000)\n",
    "df_lan = df_lan.loc[df_lan['nb_phrases_lang']>=2000]\n",
    "list_lan = list(set(df_lan.index))\n",
    "save_list_lan(list_lan)\n",
    "\n",
    "df_train = df_train[df_train['lan_code'].isin(list_lan)]\n",
    "df_test = df_test[df_test['lan_code'].isin(list_lan)]\n",
    "print('df_train:')\n",
    "display(df_train)\n",
    "print('Nombre de langues à classer:',len(list_lan))\n",
    "print('Nombre de lignes par langue:')\n",
    "display(df_lan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e81019-c0bb-4ea0-b4f9-c046cbfa93df",
   "metadata": {},
   "source": [
    "#### **Selection du Tokenizer,**\n",
    "#### **Encodage et padding du text avec le tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e39d9e7-a9fe-4b0f-86e0-fb40cca62c20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Selection du tokenizer\n",
    "if sel_tokenization==3:\n",
    "    import tiktoken\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "elif sel_tokenization==2:\n",
    "    from transformers import BertTokenizerFast\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')\n",
    "else:\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(df['sentence'])\n",
    "\n",
    "# Données d'exemple (textes et leurs langues correspondantes)\n",
    "textes = df_train['sentence']    \n",
    "langues = df_train['lan_code']\n",
    "    \n",
    "# Encodage des étiquettes (langues)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list_lan)\n",
    "labels_encoded = label_encoder.transform(langues)\n",
    "    \n",
    "def encode_text(textes):\n",
    "    global max_length, nb_unique_tokens\n",
    "    \n",
    "    if sel_tokenization==3:\n",
    "        sequences = tokenizer.encode_batch(textes)\n",
    "        nb_unique_tokens = tokenizer.max_token_value + 1\n",
    "    elif sel_tokenization==2:\n",
    "        textes = textes.tolist()\n",
    "        sequences = tokenizer.batch_encode_plus(textes).input_ids\n",
    "        nb_unique_tokens = len(set(tokenizer.get_vocab()))\n",
    "    else:\n",
    "        sequences = tokenizer.texts_to_sequences(textes)\n",
    "        nb_unique_tokens = len(tokenizer.word_index)\n",
    "    return pad_sequences(sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229d4ea-7d97-4cf7-934a-667ee8d9fa3a",
   "metadata": {},
   "source": [
    "#### **Definition du modèle d'identification et encodage de l'ensemble Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "142c27ce-60e2-4a4b-b73e-57d75b509efe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens uniques : 100277\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "# Padding des séquences\n",
    "padded_sequences = encode_text(textes) # pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "print(\"Nombre de tokens uniques :\",nb_unique_tokens)\n",
    "print(\"======\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26930276-bbda-4f31-b94a-ad05e4ebc7ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 250, 200)          20055400  \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 200)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 400)               80400     \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 200)               80200     \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 95)                9595      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,255,795\n",
      "Trainable params: 20,255,795\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Conversion des étiquettes en catégories one-hot\n",
    "labels_one_hot = to_categorical(labels_encoded)\n",
    "\n",
    "# Création du modèle\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=nb_unique_tokens, output_dim=200, input_length=max_length))\n",
    "model.add(GlobalAveragePooling1D())  \n",
    "model.add(Dense(units = 400, activation = \"tanh\", kernel_initializer='glorot_uniform', name = \"Dense_1\"))\n",
    "model.add(Dense(units = 200, activation = \"tanh\", kernel_initializer='glorot_uniform', name = \"Dense_2\"))\n",
    "model.add(Dense(units = 100, activation = \"tanh\", kernel_initializer='glorot_uniform', name = \"Dense_3\"))\n",
    "model.add(Dense(units = 100, activation = \"tanh\", kernel_initializer='glorot_uniform', name = \"Dense_4\"))\n",
    "model.add(Dense(len(df_lan), activation='softmax')) \n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ed62a-02dc-4739-a112-bff0587ab4a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Entraînement du modèle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d06717-b28d-4ab7-8e23-26cc2c4f16a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "8577/8577 [==============================] - 1786s 208ms/step - loss: 0.4460 - accuracy: 0.8713 - val_loss: 0.1825 - val_accuracy: 0.9415\n",
      "Epoch 2/40\n",
      "8577/8577 [==============================] - 1715s 200ms/step - loss: 0.1407 - accuracy: 0.9531 - val_loss: 0.1204 - val_accuracy: 0.9585\n",
      "Epoch 3/40\n",
      "8577/8577 [==============================] - 1718s 200ms/step - loss: 0.1093 - accuracy: 0.9618 - val_loss: 0.1049 - val_accuracy: 0.9634\n",
      "Epoch 4/40\n",
      "8577/8577 [==============================] - 1710s 199ms/step - loss: 0.0927 - accuracy: 0.9668 - val_loss: 0.0969 - val_accuracy: 0.9657\n",
      "Epoch 5/40\n",
      "8577/8577 [==============================] - 1717s 200ms/step - loss: 0.0837 - accuracy: 0.9695 - val_loss: 0.0856 - val_accuracy: 0.9694\n",
      "Epoch 6/40\n",
      "8577/8577 [==============================] - 1707s 199ms/step - loss: 0.0778 - accuracy: 0.9714 - val_loss: 0.0821 - val_accuracy: 0.9705\n",
      "Epoch 7/40\n",
      "8577/8577 [==============================] - 1721s 201ms/step - loss: 0.0737 - accuracy: 0.9728 - val_loss: 0.0848 - val_accuracy: 0.9700\n",
      "Epoch 8/40\n",
      "8577/8577 [==============================] - 1710s 199ms/step - loss: 0.0701 - accuracy: 0.9739 - val_loss: 0.0777 - val_accuracy: 0.9719\n",
      "Epoch 9/40\n",
      "8577/8577 [==============================] - 1715s 200ms/step - loss: 0.0674 - accuracy: 0.9749 - val_loss: 0.0763 - val_accuracy: 0.9726\n",
      "Epoch 10/40\n",
      "8577/8577 [==============================] - 1725s 201ms/step - loss: 0.0647 - accuracy: 0.9758 - val_loss: 0.0769 - val_accuracy: 0.9723\n",
      "Epoch 11/40\n",
      "8577/8577 [==============================] - 1709s 199ms/step - loss: 0.0625 - accuracy: 0.9765 - val_loss: 0.0744 - val_accuracy: 0.9735\n",
      "Epoch 12/40\n",
      "8577/8577 [==============================] - 1723s 201ms/step - loss: 0.0605 - accuracy: 0.9772 - val_loss: 0.0732 - val_accuracy: 0.9736\n",
      "Epoch 13/40\n",
      "8577/8577 [==============================] - 1716s 200ms/step - loss: 0.0587 - accuracy: 0.9779 - val_loss: 0.0746 - val_accuracy: 0.9736\n",
      "Epoch 14/40\n",
      "8577/8577 [==============================] - 1720s 200ms/step - loss: 0.0571 - accuracy: 0.9785 - val_loss: 0.0723 - val_accuracy: 0.9743\n",
      "Epoch 15/40\n",
      "8577/8577 [==============================] - 1759s 205ms/step - loss: 0.0557 - accuracy: 0.9789 - val_loss: 0.0753 - val_accuracy: 0.9736\n",
      "Epoch 16/40\n",
      "8577/8577 [==============================] - 2002s 233ms/step - loss: 0.0544 - accuracy: 0.9795 - val_loss: 0.0700 - val_accuracy: 0.9752\n",
      "Epoch 17/40\n",
      "8577/8577 [==============================] - 1744s 203ms/step - loss: 0.0531 - accuracy: 0.9799 - val_loss: 0.0712 - val_accuracy: 0.9751\n",
      "Epoch 18/40\n",
      "8577/8577 [==============================] - 1717s 200ms/step - loss: 0.0520 - accuracy: 0.9803 - val_loss: 0.0721 - val_accuracy: 0.9751\n",
      "Epoch 19/40\n",
      "8577/8577 [==============================] - 1746s 204ms/step - loss: 0.0509 - accuracy: 0.9807 - val_loss: 0.0720 - val_accuracy: 0.9750\n",
      "Epoch 20/40\n",
      "8577/8577 [==============================] - 1767s 206ms/step - loss: 0.0499 - accuracy: 0.9811 - val_loss: 0.0706 - val_accuracy: 0.9755\n",
      "Epoch 21/40\n",
      "8577/8577 [==============================] - 1837s 214ms/step - loss: 0.0489 - accuracy: 0.9814 - val_loss: 0.0720 - val_accuracy: 0.9748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c8d8252a00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Entraînement du modèle\n",
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True,)\n",
    "checkpoint = ModelCheckpoint('best_model.h5',\n",
    "                             monitor='val_accuracy',\n",
    "                             save_best_only=True,\n",
    "                             # save_weights_only=True,\n",
    "                             mode='max',\n",
    "                             verbose=1)\n",
    "model.fit(padded_sequences, labels_one_hot, epochs=40, validation_split=0.1, batch_size=1024, verbose=1, callbacks=[stop_early,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02af7fc0-07a7-480e-a55a-7ae5a06a426b",
   "metadata": {},
   "source": [
    "#### **Sauvegarde et/ou Chargement du modele**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8ad6635-e2a5-4cc0-b809-c2f6d85b4ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# definition du nom de fichier de sauvegarde\n",
    "def get_file_name(sel_tokenization):\n",
    "    if sel_tokenization == 3: return(\"../data/dl_tiktoken_id_language_model_big.h5\")\n",
    "    elif sel_tokenization == 2: return(\"../data/dl_BERT_id_language_model_big.h5\")\n",
    "    else: return(\"../data/dl_default_id_language_model_big.h5\")\n",
    "\n",
    "\n",
    "# Sauvegarde du modèle entrainé\n",
    "# model.save(get_file_name(sel_tokenization))\n",
    "\n",
    "# Voici une instruction qui permet de passer sous la barre des 100 Mo.....\n",
    "# from filesplit.split import Split\n",
    "# Split(get_file_name(sel_tokenization),\"../data/dl_id_lang_split\").bysize(66846720)\n",
    "if sel_tokenization==1:\n",
    "    with open('../data/tokenizer_Keras.pkl', 'wb') as tokenizer_file:\n",
    "        pickle.dump(tokenizer, tokenizer_file)\n",
    "\n",
    "\n",
    "# Chargement du tokenizer Keras pré-entrainé\n",
    "from filesplit.merge import Merge\n",
    "merge = Merge(\"../data/dl_id_lang_split\",  \"../data\", get_file_name(sel_tokenization)).merge(cleanup=False)\n",
    "model = keras.models.load_model(get_file_name(sel_tokenization))\n",
    "if sel_tokenization==1:\n",
    "    with open('../data/tokenizer_Keras.pkl', 'rb') as tokenizer_file:\n",
    "        tokenizer = pickle.load(tokenizer_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1752508-bfa0-4d06-9f52-b13c52a931af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 250, 200)          20055400  \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 200)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 400)               80400     \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 200)               80200     \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 95)                9595      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,255,795\n",
      "Trainable params: 20,255,795\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6457401a-49ae-4fda-b025-3d0b68994253",
   "metadata": {},
   "source": [
    "#### **Test de l'efficacité du modèle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4b38164-7529-4227-9409-c2daa5ed22ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16049/16049 [==============================] - 59s 4ms/step\n",
      "======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         afr       0.92      0.83      0.88       218\n",
      "         ara       0.98      0.99      0.98      1930\n",
      "         arq       0.88      0.65      0.75       112\n",
      "         asm       0.95      0.84      0.90       173\n",
      "         avk       0.91      0.76      0.83       207\n",
      "         aze       0.98      0.84      0.90       273\n",
      "         bel       0.99      0.81      0.89       609\n",
      "         ben       0.91      0.98      0.94       266\n",
      "         ber       0.88      0.91      0.90     29271\n",
      "         bre       0.95      0.86      0.90       380\n",
      "         bul       0.90      0.81      0.85      1291\n",
      "         cat       0.91      0.81      0.86       426\n",
      "         cbk       0.86      0.80      0.83       115\n",
      "         ces       0.97      0.96      0.96      3159\n",
      "         ckb       0.99      0.99      0.99       494\n",
      "         cmn       0.98      0.98      0.98      3625\n",
      "         cor       0.98      0.93      0.95       220\n",
      "         dan       0.91      0.94      0.92      2717\n",
      "         deu       1.00      0.99      1.00     29375\n",
      "         dtp       0.79      0.66      0.72       145\n",
      "         ell       1.00      1.00      1.00      1706\n",
      "         eng       1.00      1.00      1.00     79127\n",
      "         epo       0.99      1.00      1.00     34366\n",
      "         est       0.90      0.78      0.84       185\n",
      "         eus       0.91      0.88      0.90       308\n",
      "         fin       0.98      0.99      0.98      6846\n",
      "         fra       0.99      1.00      1.00     24937\n",
      "         frr       0.91      0.90      0.91       143\n",
      "         gcf       0.92      0.78      0.85       116\n",
      "         gle       0.99      0.83      0.90       124\n",
      "         glg       0.62      0.76      0.68       246\n",
      "         gos       0.87      0.77      0.82       284\n",
      "         grn       0.93      0.91      0.92       157\n",
      "         heb       1.00      1.00      1.00      9823\n",
      "         hin       0.97      0.96      0.97       749\n",
      "         hrv       0.38      0.60      0.47       256\n",
      "         hun       1.00      0.99      0.99     17738\n",
      "         hye       1.00      1.00      1.00       234\n",
      "         ido       0.93      0.82      0.87       511\n",
      "         ile       0.96      0.74      0.84       405\n",
      "         ilo       0.89      0.82      0.85       116\n",
      "         ina       0.93      0.94      0.93      1413\n",
      "         ind       0.78      0.88      0.83       780\n",
      "         isl       0.99      0.95      0.97       624\n",
      "         ita       0.99      1.00      0.99     40576\n",
      "         jbo       1.00      0.97      0.98       863\n",
      "         jpn       1.00      1.00      1.00     11106\n",
      "         kab       0.89      0.87      0.88     25844\n",
      "         kat       1.00      1.00      1.00       309\n",
      "         kaz       0.97      0.92      0.94       183\n",
      "         kmr       0.98      0.95      0.96       459\n",
      "         kor       1.00      1.00      1.00       463\n",
      "         kzj       0.80      0.90      0.85       233\n",
      "         lat       0.94      0.96      0.95      2081\n",
      "         lfn       0.90      0.95      0.92      1066\n",
      "         lit       0.98      0.98      0.98      3867\n",
      "         lvs       0.97      0.83      0.90       125\n",
      "         mar       0.99      0.99      0.99      3504\n",
      "         mhr       0.92      0.91      0.92       211\n",
      "         mkd       0.87      0.94      0.90      3808\n",
      "         mon       0.98      0.90      0.94       153\n",
      "         nds       0.91      0.95      0.93       887\n",
      "         nld       0.99      0.98      0.98      7993\n",
      "         nob       0.75      0.78      0.77       716\n",
      "         oci       0.89      0.91      0.90       289\n",
      "         ota       0.91      0.63      0.74       106\n",
      "         pes       0.99      0.99      0.99      1321\n",
      "         pol       0.99      0.99      0.99      5869\n",
      "         por       1.00      0.98      0.99     20087\n",
      "         ron       0.95      0.98      0.96      1444\n",
      "         run       0.94      0.87      0.90       265\n",
      "         rus       0.99      0.99      0.99     45522\n",
      "         shi       0.89      0.56      0.69       106\n",
      "         slk       0.92      0.85      0.88       864\n",
      "         spa       0.98      0.99      0.98     18509\n",
      "         sqi       0.98      0.89      0.93       143\n",
      "         srp       0.84      0.76      0.80      2286\n",
      "         swe       0.96      0.96      0.96      2372\n",
      "         tat       0.98      0.93      0.96       649\n",
      "         tgl       0.98      0.95      0.96       880\n",
      "         tha       1.00      1.00      1.00       226\n",
      "         tlh       0.98      0.98      0.98      1180\n",
      "         tok       1.00      1.00      1.00      2355\n",
      "         tuk       0.98      0.88      0.93       345\n",
      "         tur       1.00      1.00      1.00     35905\n",
      "         uig       0.99      0.99      0.99       406\n",
      "         ukr       0.95      0.97      0.96      8931\n",
      "         urd       1.00      0.98      0.99       101\n",
      "         vie       1.00      1.00      1.00      1049\n",
      "         vol       0.98      0.89      0.93       230\n",
      "         war       0.92      0.83      0.87       111\n",
      "         wuu       0.81      0.79      0.80       254\n",
      "         yid       0.99      0.95      0.97       463\n",
      "         yue       0.91      0.88      0.89       329\n",
      "         zsm       0.69      0.62      0.65       321\n",
      "\n",
      "    accuracy                           0.97    513565\n",
      "   macro avg       0.93      0.90      0.91    513565\n",
      "weighted avg       0.98      0.97      0.97    513565\n",
      "\n",
      "======\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Classe prédite</th>\n",
       "      <th>afr</th>\n",
       "      <th>ara</th>\n",
       "      <th>arq</th>\n",
       "      <th>asm</th>\n",
       "      <th>avk</th>\n",
       "      <th>aze</th>\n",
       "      <th>bel</th>\n",
       "      <th>ben</th>\n",
       "      <th>ber</th>\n",
       "      <th>bre</th>\n",
       "      <th>...</th>\n",
       "      <th>uig</th>\n",
       "      <th>ukr</th>\n",
       "      <th>urd</th>\n",
       "      <th>vie</th>\n",
       "      <th>vol</th>\n",
       "      <th>war</th>\n",
       "      <th>wuu</th>\n",
       "      <th>yid</th>\n",
       "      <th>yue</th>\n",
       "      <th>zsm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classe réelle</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>afr</th>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ara</th>\n",
       "      <td>0</td>\n",
       "      <td>1912</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arq</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avk</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>war</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wuu</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yid</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>440</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yue</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Classe prédite  afr   ara  arq  asm  avk  aze  bel  ben  ber  bre  ...  uig  \\\n",
       "Classe réelle                                                      ...        \n",
       "afr             182     0    0    0    0    0    0    0    1    0  ...    0   \n",
       "ara               0  1912    9    0    0    0    0    0    1    0  ...    1   \n",
       "arq               0    34   73    0    0    0    0    0    1    0  ...    0   \n",
       "asm               0     0    0  146    0    0    0   27    0    0  ...    0   \n",
       "avk               0     0    0    0  158    0    0    0    3    3  ...    0   \n",
       "...             ...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "war               0     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "wuu               0     1    0    0    0    0    0    0    0    0  ...    0   \n",
       "yid               0     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "yue               0     0    1    0    0    0    0    0    0    0  ...    0   \n",
       "zsm               0     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "\n",
       "Classe prédite  ukr  urd  vie  vol  war  wuu  yid  yue  zsm  \n",
       "Classe réelle                                                \n",
       "afr               0    0    0    1    0    0    0    0    0  \n",
       "ara               0    0    0    0    0    0    0    0    0  \n",
       "arq               0    0    0    0    0    0    0    0    0  \n",
       "asm               0    0    0    0    0    0    0    0    0  \n",
       "avk               0    0    0    0    1    0    0    0    1  \n",
       "...             ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "war               0    0    0    0   92    0    0    0    0  \n",
       "wuu               0    0    0    0    0  200    0    5    0  \n",
       "yid               0    0    0    0    0    0  440    0    0  \n",
       "yue               0    0    0    0    0    2    0  288    0  \n",
       "zsm               0    0    0    0    0    0    0    0  199  \n",
       "\n",
       "[95 rows x 95 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Classifier = 0.975\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
    "import random\n",
    "\n",
    "# Préparation des nouvelles données à prédire\n",
    "textes_test = df_test['sentence']\n",
    "langues_test = df_test['lan_code']\n",
    "\n",
    "# Prédiction des langues des nouveaux textes\n",
    "predictions = model.predict(encode_text(textes_test))\n",
    "\n",
    "# Décodage des prédictions en langues\n",
    "predicted_labels_encoded = np.argmax(predictions, axis=1)\n",
    "predicted_languages = label_encoder.classes_[predicted_labels_encoded]\n",
    "print(\"======\")\n",
    "\n",
    "print(classification_report(langues_test,predicted_languages))\n",
    "print(\"======\")\n",
    "\n",
    "ct = pd.crosstab(langues_test,predicted_languages,rownames=['Classe réelle'], colnames=['Classe prédite'])\n",
    "display(ct)\n",
    "accuracy_clf = accuracy_score(langues_test,predicted_languages)\n",
    "print(\"Accuracy Classifier = {:.3f}\".format(accuracy_clf))\n",
    "print(\"======\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e27633f3-e0eb-44a1-9d6a-459d127a0d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Classe prédite</th>\n",
       "      <th>deu</th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>ita</th>\n",
       "      <th>spa</th>\n",
       "      <th>etc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classe réelle</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deu</th>\n",
       "      <td>29225</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng</th>\n",
       "      <td>2</td>\n",
       "      <td>79035</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fra</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>24864</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>40442</td>\n",
       "      <td>20</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spa</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>18322</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etc</th>\n",
       "      <td>39</td>\n",
       "      <td>84</td>\n",
       "      <td>104</td>\n",
       "      <td>302</td>\n",
       "      <td>358</td>\n",
       "      <td>320154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Classe prédite    deu    eng    fra    ita    spa     etc\n",
       "Classe réelle                                            \n",
       "deu             29225     10      5      3      1     131\n",
       "eng                 2  79035      8      6      6      70\n",
       "fra                 1      4  24864     17      6      45\n",
       "ita                 0      1     10  40442     20     103\n",
       "spa                 0      0      4     44  18322     139\n",
       "etc                39     84    104    302    358  320154"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy pour les langues d'Europe de l'Ouest = 0.992\n"
     ]
    }
   ],
   "source": [
    "ct_weurope = pd.DataFrame(ct)\n",
    "lignes_a_sommer = ct.index.difference(['eng','deu','fra','ita','spa'])\n",
    "somme_l = ct.loc[lignes_a_sommer].sum()\n",
    "ct_weurope.loc['etc'] = somme_l\n",
    "ct_weurope = ct_weurope.drop(index = ct_weurope.index.difference(['eng','deu','fra','ita','spa','etc']))\n",
    "\n",
    "colonnes_a_sommer = ct_weurope.columns.difference(['eng','deu','fra','ita','spa'])\n",
    "somme_c = ct_weurope[colonnes_a_sommer].sum(axis=1)\n",
    "ct_weurope['etc']= somme_c\n",
    "ct_weurope = ct_weurope.drop(columns=ct_weurope.columns.difference(['eng','deu','fra','ita','spa','etc']))\n",
    "display(ct_weurope)\n",
    "accuracy_weurope = (ct_weurope['eng']['eng']+ct_weurope['deu']['deu']+ct_weurope['fra']['fra']+ct_weurope['ita']['ita']+ct_weurope['spa']['spa'])/(ct_weurope.sum().sum()-ct_weurope['etc']['etc'])\n",
    "print(\"Accuracy pour les langues d'Europe de l'Ouest = {:.3f}\".format(accuracy_weurope))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f2f2d-9b2b-4dbf-8df3-57b45cf2918d",
   "metadata": {},
   "source": [
    "#### **Affichage d'exemples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "683d531f-fdf1-4e6d-bb23-c583ca89f3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemples de prédiction de langue:\n",
      "Réelle\t- Prédite - Texte\n",
      " ita\t- ita     -'Sto leggendo il libro 'La lingua pericolosa' di Ulrich Lins.............................................................'\n",
      " lit\t- lit     -'Taip, tai yra labai gražu. Kiek tai kainuoja?...........................................................................'\n",
      " eng\t- eng     -'The man gave a big cry..................................................................................................'\n",
      " tur\t- tur     -'Burada bekle. Ben kısa zamanda dönerim..................................................................................'\n",
      " ita\t- ita     -'Perché mi sospetti?.....................................................................................................'\n",
      " por\t- por     -'Não sei a quem o darei..................................................................................................'\n",
      " rus\t- rus     -'Отведите их туда........................................................................................................'\n",
      " tur\t- tur     -'Beyazdı.................................................................................................................'\n",
      " kab\t- kab     -'Bubbent ar At Wuccen....................................................................................................'\n",
      " spa\t- spa     -'Los atracadores acribillaron a tiros a los agentes que se aproximaron al edificio.......................................'\n"
     ]
    }
   ],
   "source": [
    "# Affichage des prédictions\n",
    "print(\"Exemples de prédiction de langue:\")\n",
    "print(\"Réelle\\t- Prédite - Texte\")\n",
    "n_test = min(len(textes_test),10)\n",
    "for _ in range(n_test):\n",
    "    i = random.randint(0, len(textes_test))\n",
    "    print(f\" {langues_test.iloc[i]}\\t- {predicted_languages[i]}     -'{textes_test.iloc[i].ljust(120, '.')[:120]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "869fd798-514f-49e4-8253-21a7b519da9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemples de mauvaises prédictions de langue:\n",
      "Réelle\t- Prédite - Texte\n",
      " deu\t- dan     -'Hilft Tom Mary?.........................................................................................................'\n",
      " ber\t- kab     -'Iruka-a n yemma.........................................................................................................'\n",
      " kab\t- ber     -'Lliɣ ttnaɣeɣ d yiḍ, tawla, iḍes asemmiḍ.................................................................................'\n",
      " por\t- glg     -'É esta a chave que estás buscando?......................................................................................'\n",
      " slk\t- ces     -'Je odo mňa o hodne vyššia...............................................................................................'\n",
      " ota\t- uig     -'اونڭ بابه سى وار........................................................................................................'\n",
      " kab\t- ber     -'Σerḍeɣ-t-id maca ɣas akken, ur d-yusa ara...............................................................................'\n",
      " ukr\t- rus     -'Том одягнув шкарпетки...................................................................................................'\n",
      " bul\t- mkd     -'На кого е това палто?...................................................................................................'\n",
      " rus\t- ukr     -'Я принимаю наркотики....................................................................................................'\n"
     ]
    }
   ],
   "source": [
    "# Affichage de mauvaises prédictions\n",
    "print(\"Exemples de mauvaises prédictions de langue:\")\n",
    "list_bad = []\n",
    "n = len(textes_test)\n",
    "if n>0:\n",
    "    for i in range(n):\n",
    "        if predicted_languages[i] != langues_test.iloc[i] :\n",
    "            list_bad.append(i)\n",
    "    print(\"Réelle\\t- Prédite - Texte\")\n",
    "    n_test = min(n,10)\n",
    "    for _ in range(n_test):\n",
    "        i = random.randint(0, len(list_bad))\n",
    "        print(f\" {langues_test.iloc[list_bad[i]]}\\t- {predicted_languages[list_bad[i]]}     -'{textes_test.iloc[list_bad[i]].ljust(120, '.')[:120]}'\")\n",
    "else:\n",
    "    print(\"Félicitations !!!! Le modèle n'a fait aucune mauvaise prédictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1c4cd207-8427-4135-9b96-cdd0977629ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 88ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'French'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lang_id_dl(sentences):\n",
    "    \n",
    "    if \"str\" in str(type(sentences)): predictions = model.predict(encode_text([sentences]))\n",
    "    else:  predictions = model.predict(encode_text(sentences))\n",
    "    # Décodage des prédictions en langues\n",
    "    predicted_labels_encoded = np.argmax(predictions, axis=1)\n",
    "    predicted_languages = label_encoder.classes_[predicted_labels_encoded]\n",
    "    if \"str\" in str(type(sentences)): return lan_to_language[predicted_languages[0]]\n",
    "    else: return [l for l in predicted_languages]\n",
    "\n",
    "lang_id_dl(\"Afin de mettre en oeuvre cette fonctionnalité \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99ec7262-ace0-4c2e-a72c-9f0713a314ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no 510000  -  9.855663537979126 s ( 3.0108154584438068e-05 s/id )           \n",
      "Nombre de phrases prises en compte : 329683\n",
      "Durée de traitement : 9.925775289535522 secondes, c'est à dire  3.0107027931484252e-05 s/identification\n",
      "Accuracy du Deep Learnings : 0.992441223842297\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "y_ext_actual = []\n",
    "y_dl_predicted=[]\n",
    "dict_xlmr  = {\"ar\":\"ara\", \"bg\":\"bul\", \"de\":\"deu\", \"el\": \"ell\", \"en\":\"eng\", \"es\":\"spa\", \"fr\":\"fra\", \"hi\": \"hin\",\"it\":\"ita\",\"ja\":\"jpn\", \\\n",
    "              \"nl\":\"nld\", \"pl\":\"pol\", \"pt\":\"por\", \"ru\":\"rus\", \"sw\":\"swh\", \"th\":\"tha\", \"tr\":\"tur\", \"ur\": \"urd\", \"vi\":\"vie\", \"zh\":\"cmn\"}\n",
    "lang_available = list(dict_xlmr.values())\n",
    "start_time = time.time()\n",
    "j= 0\n",
    "for i in range(len(df_test)):\n",
    "    if df_test.lan_code.iloc[i] in lang_available:\n",
    "        y_ext_actual.append(df_test.lan_code.iloc[i])\n",
    "        y_dl_predicted.append(predicted_languages[i])\n",
    "        if (i-j)>=10000:\n",
    "            j = (i//10000)*10000\n",
    "            d = (time.time()- start_time)\n",
    "            print(\"no\",j,\" - \",d,\"s (\",d/len(y_ext_actual),\"s/id )          \",end=\"\\r\")\n",
    "            \n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(\"\")\n",
    "print(\"Nombre de phrases prises en compte :\",len(y_ext_actual))\n",
    "print(\"Durée de traitement :\",duration,\"secondes, c'est à dire \", duration/len(y_ext_actual),\"s/identification\")\n",
    "print(\"Accuracy du Deep Learnings :\",accuracy_score(y_ext_actual, y_dl_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987a700a-568f-420a-bf5f-19bc4c34d179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
