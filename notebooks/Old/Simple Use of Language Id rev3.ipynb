{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a996d864-6a89-4513-95ba-2edb457d25be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Utilisation simple des identifieurs de 95 langues** (rev3) **avec les :**\n",
    ">### **- '*Sparse*' Bag Of Words**\n",
    ">### **- Tokenisations Tiktoken**\n",
    ">### **- CountVectorizer utilisant une tokenisation '*custom*'**\n",
    ">### **- Classificateur Naïve Bayesg**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e67500-a4c1-4d58-8f4a-1be20434be6d",
   "metadata": {},
   "source": [
    "#### **Choix du tokenizer** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb14523-04a8-424c-976a-d61585c0bbf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "import random\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Ce parametre permet éventuellement d'équilibrer de nombre de phrase par langue.\n",
    "# Si ce parametre est très grand, tout le corpus sera lu. \n",
    "nb_phrase_lang = 500000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bad4ba-8678-41a4-9008-ab1915eddae6",
   "metadata": {},
   "source": [
    "#### **Lectures des phrases de \"sentences-big.csv\", et de leur étiquette \"Langue\" pour les langues sélectionnées**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "491d66b2-90bd-49e9-99cb-9601edf52a57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes de sentence.csv: 10341812\n",
      "Nombre de langues à classer: 404\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ita</td>\n",
       "      <td>Il tuo futuro è pieno di possibilità.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fra</td>\n",
       "      <td>J'aimerais aller en France, un jour.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>epo</td>\n",
       "      <td>La polica enketo aperigis ilian sekretan vivon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kab</td>\n",
       "      <td>Kullec ifukk yid-k.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hun</td>\n",
       "      <td>Több munkát nem tudok elvállalni.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>epo</td>\n",
       "      <td>Tiuj amikoj havas malbonan influon sur vi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>por</td>\n",
       "      <td>Se ao menos eu soubesse!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kab</td>\n",
       "      <td>Kemm d yiwet seg timeddukal n Tom, neɣ ala?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fra</td>\n",
       "      <td>Augmente le son.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hun</td>\n",
       "      <td>Olyan keményen dolgoztam, amennyire csak lehet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lan_code                                           sentence\n",
       "0      ita              Il tuo futuro è pieno di possibilità.\n",
       "1      fra               J'aimerais aller en France, un jour.\n",
       "2      epo    La polica enketo aperigis ilian sekretan vivon.\n",
       "3      kab                                Kullec ifukk yid-k.\n",
       "4      hun                  Több munkát nem tudok elvállalni.\n",
       "5      epo         Tiuj amikoj havas malbonan influon sur vi.\n",
       "6      por                           Se ao menos eu soubesse!\n",
       "7      kab        Kemm d yiwet seg timeddukal n Tom, neɣ ala?\n",
       "8      fra                                   Augmente le son.\n",
       "9      hun  Olyan keményen dolgoztam, amennyire csak lehet..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10341802</th>\n",
       "      <td>hun</td>\n",
       "      <td>Úgy gondolom, nagyon jó a munkád.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341803</th>\n",
       "      <td>eng</td>\n",
       "      <td>We had our doubts.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341804</th>\n",
       "      <td>ita</td>\n",
       "      <td>Va a imparare qualcosa in Irlanda?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341805</th>\n",
       "      <td>mar</td>\n",
       "      <td>मी रांगेत शेवटी होते.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341806</th>\n",
       "      <td>ber</td>\n",
       "      <td>Nṛuḥ ad d-naɣ imsured aɣurbiz.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341807</th>\n",
       "      <td>deu</td>\n",
       "      <td>Wir werden das Problem nicht aufgreifen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341808</th>\n",
       "      <td>fra</td>\n",
       "      <td>Je suis cuit !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341809</th>\n",
       "      <td>kab</td>\n",
       "      <td>Isefk fell-ak a tregleḍ iɣis-a.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341810</th>\n",
       "      <td>tok</td>\n",
       "      <td>o pana ala e moku tawa soweli tomo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341811</th>\n",
       "      <td>hun</td>\n",
       "      <td>Máris unod magad?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lan_code                                  sentence\n",
       "10341802      hun         Úgy gondolom, nagyon jó a munkád.\n",
       "10341803      eng                        We had our doubts.\n",
       "10341804      ita        Va a imparare qualcosa in Irlanda?\n",
       "10341805      mar                     मी रांगेत शेवटी होते.\n",
       "10341806      ber            Nṛuḥ ad d-naɣ imsured aɣurbiz.\n",
       "10341807      deu  Wir werden das Problem nicht aufgreifen.\n",
       "10341808      fra                            Je suis cuit !\n",
       "10341809      kab           Isefk fell-ak a tregleḍ iɣis-a.\n",
       "10341810      tok       o pana ala e moku tawa soweli tomo.\n",
       "10341811      hun                         Máris unod magad?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ouvrir le fichier d'entrée en mode lecture\n",
    "def create_lang_df(path):\n",
    "    df = pd.read_csv(path, index_col ='id')\n",
    "    return df\n",
    "\n",
    "df = create_lang_df('../data/multilingue/sentences-big.csv')\n",
    "lan_code = list(set(df['lan_code']))\n",
    "df_lan = pd.DataFrame(data= df.groupby('lan_code').size(), columns = ['nb_phrases_lang'] )\n",
    "\n",
    "# Filtrage des langues qui ont peu de phrases (>2000)\n",
    "df_lan = df_lan.loc[df_lan['nb_phrases_lang']>=2000]\n",
    "list_lan = list(set(df_lan.index))\n",
    "\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "n_rows = len(df)\n",
    "print('Nombre de lignes de sentence.csv:',n_rows)\n",
    "print('Nombre de langues à classer:',len(lan_code))\n",
    "display(df.head(10))\n",
    "display(df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb9b00-6cc3-47bc-afcf-b00fdef8738b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Préparation de la vectorisation par CountVectorizer** <font color='red'>(nécéssaire pour traduction texte libre)</font>\n",
    "#### **Création de la fonction de création d'un Bags  Of Worlds pour les phrases à identifier** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df235a48-3ea5-47a7-af0a-f91fd20af84e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selection du tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Les 2 fonctions suivantes sont nécéssaires afin de sérialiser ces parametre de CountVectorizer\n",
    "# et ainsi de sauvegarder le vectorizer pour un un usage ultérieur sans utiliser X_train pour  le réinitialiser\n",
    "def custom_tokenizer(text):\n",
    "    tokens = tokenizer.encode(text)  # Cela divise le texte en mots\n",
    "    return tokens\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    return text\n",
    "\n",
    "# CountVectorizer a une liste de phrase en entrée.\n",
    "# Cette fonction met les données d'entrée dans le bon format\n",
    "def format_to_vectorize(data):\n",
    "    X_tok = []\n",
    "    if \"DataFrame\" in str(type(data)):sentences = df.tolist()\n",
    "    elif \"str\" in str(type(data)):\n",
    "        sentences =[data]\n",
    "    else: sentences = data\n",
    "                          \n",
    "    for sentence in sentences:\n",
    "        X_tok.append(sentence) # ('¤'.join([tokenizer.decode([ids]) for ids in tokenizer.encode(sentence)])+'¤')\n",
    "    return X_tok\n",
    "\n",
    "def create_BOW(data):\n",
    "    global vectorizer\n",
    "    \n",
    "    X_tok = format_to_vectorize(data)\n",
    "    X = vectorizer.transform(X_tok)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf34d21-5bde-4d0f-a149-0ad5a48a39ed",
   "metadata": {},
   "source": [
    "#### **Chargement du vectorizer** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "773d2983-dd0a-450a-af97-a5d3034fe315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectorizer():\n",
    "    global dict_token, dict_ids, nb_token\n",
    "    \n",
    "    path = '../data/vectorizer_tiktoken_big.pkl'\n",
    "    vectorizer = joblib.load(path)\n",
    "    dict_token = {tokenizer.decode([cle]): cle for cle, valeur in vectorizer.vocabulary_.items()}\n",
    "    dict_ids = {cle: tokenizer.decode([cle]) for cle, valeur in vectorizer.vocabulary_.items()} #dict_ids.items()}\n",
    "    nb_token = len(vectorizer.vocabulary_)\n",
    "    return vectorizer\n",
    "\n",
    "vectorizer = load_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923233dd-f03a-400a-bc7c-b5fc4914ca8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Chargement du classificateur entrainé avec l'algorithme Naïve Bayes** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54b3449f-f635-402a-b3bf-5c163fadad52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn import naive_bayes\n",
    "\n",
    "# Chargement du classificateur sauvé\n",
    "clf_nb = joblib.load(\"../data/id_lang_tiktoken_nb_sparse_big.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de709b0-83c2-4b3f-a018-ea6c10531f99",
   "metadata": {},
   "source": [
    "#### **Definition de fonctions identificateur de langue avec Naive Bayes** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "753c88f7-991f-4097-af13-6bae4f1e4090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Chargement du classificateur sauvegardé\n",
    "clf_nb = joblib.load(\"../data/id_lang_tiktoken_nb_sparse_big.pkl\")\n",
    "vectorizer = load_vectorizer()\n",
    "\n",
    "# Lisez le contenu du fichier JSON\n",
    "with open('../data/multilingue/lan_to_language.json', 'r') as fichier:\n",
    "    lan_to_language = json.load(fichier)\n",
    "\n",
    "\n",
    "\n",
    "def lang_id_nb(sentences):\n",
    "    if \"str\" in str(type(sentences)):\n",
    "        return lan_to_language[clf_nb.predict(create_BOW(sentences))[0]]\n",
    "    else: return [lan_to_language[l] for l in clf_nb.predict(create_BOW(sentences))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cdd3d07-abf0-48e3-9801-6cb2e4263d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lang_id_gb(sentences):\n",
    "    if \"str\" in str(type(sentences)):\n",
    "        return lan_to_language[clf_gb.predict(create_BOW(sentences))[0]]\n",
    "    else:\n",
    "        return [lan_to_language[l] for l in clf_gb.predict(create_BOW(sentences))]        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82b0a7-3649-4c66-97d2-e38c52e2fbfc",
   "metadata": {},
   "source": [
    "#### **Exemples d'utilisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a73abd8-94fb-4ed5-8ad9-3308c95ca077",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\t lang\t Phrase\n",
      "0 -\t eng \t france is often snowy during spring , and it is relaxing in january .\n",
      "1 -\t fra \t elle adore les voitures très luxueuses, et toi ?\n",
      "2 -\t eng \t she loves very luxurious cars, don't you?\n",
      "3 -\t spa \t vamos a la playa\n",
      "4 -\t deu \t Ich heiße Keyne, und das ist wunderbar\n",
      "5 -\t e,f,d \t she loves you, mais elle te hait aussi, and das ist traurig\n",
      "6 -\t en \t I ate caviar\n",
      "7 -\t ita \t Vogliamo visitare il Colosseo e nuotare nel Tevere.\n",
      "8 -\t fra \t Il a dit qu'il ne connaissait pas cet homme, mais c'était un mensonge.\n",
      "9 -\t jpn \t 私はお金を全部ではないにしても、新車を買うために３分の２以上は使ってしまった。\n",
      "10 -\t deu \t Man hört nur ihre Kritik!\n",
      "11 -\t pol \t Jutro oddam ci pieniądze, które mi pożyczyłeś.\n",
      "12 -\t rus \t Он предложил помочь.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Instanciation d'exemples\n",
    "\n",
    "sentence_no = random.sample(range(len(df)),5)\n",
    "\n",
    "exemples = [\"france is often snowy during spring , and it is relaxing in january .\",\n",
    "           \"elle adore les voitures très luxueuses, et toi ?\",\n",
    "           \"she loves very luxurious cars, don't you?\",\n",
    "           \"vamos a la playa\",\n",
    "           \"Ich heiße Keyne, und das ist wunderbar\",\n",
    "           \"she loves you, mais elle te hait aussi, and das ist traurig\", # Attention à cette phrase trilingue\n",
    "           \"I ate caviar\", \n",
    "           \"Vogliamo visitare il Colosseo e nuotare nel Tevere.\",\n",
    "            df['sentence'].iloc[sentence_no[0]],\n",
    "            df['sentence'].iloc[sentence_no[1]],\n",
    "            df['sentence'].iloc[sentence_no[2]],\n",
    "            df['sentence'].iloc[sentence_no[3]],\n",
    "            df['sentence'].iloc[sentence_no[4]],\n",
    "          ]\n",
    "lang_exemples = ['eng','fra','eng','spa','deu','e,f,d','en','ita',df['lan_code'].iloc[sentence_no[0]],df['lan_code'].iloc[sentence_no[1]],df['lan_code'].iloc[sentence_no[2]],\n",
    "                 df['lan_code'].iloc[sentence_no[3]],df['lan_code'].iloc[sentence_no[4]]]\n",
    "print('no\\t lang\\t Phrase')                            \n",
    "for i in range(len(exemples)):\n",
    "    print(i,'-\\t',lang_exemples[i],'\\t',exemples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5b4a279-9aea-494e-b468-de0f024cd899",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langue réelle\tPréd. Naive B.\tPhrase\n",
      "eng\t\tEnglish\t\tfrance is often snowy during spring , and it is relaxing in january .\n",
      "fra\t\tFrench\t\telle adore les voitures très luxueuses, et toi ?\n",
      "eng\t\tEnglish\t\tshe loves very luxurious cars, don't you?\n",
      "spa\t\tSpanish\t\tvamos a la playa\n",
      "deu\t\tGerman\t\tIch heiße Keyne, und das ist wunderbar\n",
      "e,f,d\t\tGalician\t\tshe loves you, mais elle te hait aussi, and das ist traurig\n",
      "en\t\tTurkish\t\tI ate caviar\n",
      "ita\t\tItalian\t\tVogliamo visitare il Colosseo e nuotare nel Tevere.\n",
      "fra\t\tFrench\t\tIl a dit qu'il ne connaissait pas cet homme, mais c'était un mensonge.\n",
      "jpn\t\tJapanese\t\t私はお金を全部ではないにしても、新車を買うために３分の２以上は使ってしまった。\n",
      "deu\t\tGerman\t\tMan hört nur ihre Kritik!\n",
      "pol\t\tPolish\t\tJutro oddam ci pieniądze, które mi pożyczyłeś.\n",
      "rus\t\tRussian\t\tОн предложил помочь.\n"
     ]
    }
   ],
   "source": [
    "# Affichage des prédictions\n",
    "print(\"Langue réelle\\tPréd. Naive B.\\tPhrase\")\n",
    "for i in range(len(exemples)):\n",
    "    print(lang_exemples[i]+'\\t\\t'+lang_id_nb(exemples[i])[:12]+'\\t\\t'+exemples[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8d1c1-e5dd-4a17-8aee-1305fbf4c45a",
   "metadata": {},
   "source": [
    "> **Recherche des phrases mal classées par Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d458174a-c2e1-41e8-a84a-35584eaac751",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN°Ligne\tL. réelle\tPréd. Naive B.\t\tPhrase\n",
      "1 \t 0 \t- ita \t\tItalian      \t\tIl tuo futuro è pieno di possibilità.  (proba=1.00)\n",
      "2 \t 1 \t- fra \t\tFrench       \t\tJ'aimerais aller en France, un jour.  (proba=1.00)\n",
      "3 \t 2 \t- epo \t\tEsperanto    \t\tLa polica enketo aperigis ilian sekretan vivon.  (proba=1.00)\n",
      "4 \t 3 \t- kab \t\tBerber langu \t\tKullec ifukk yid-k.  (proba=0.71)\n",
      "5 \t 4 \t- hun \t\tHungarian    \t\tTöbb munkát nem tudok elvállalni.  (proba=1.00)\n",
      "6 \t 5 \t- epo \t\tEsperanto    \t\tTiuj amikoj havas malbonan influon sur vi.  (proba=1.00)\n",
      "7 \t 6 \t- por \t\tPortuguese   \t\tSe ao menos eu soubesse!  (proba=1.00)\n",
      "8 \t 7 \t- kab \t\tBerber langu \t\tKemm d yiwet seg timeddukal n Tom, neɣ ala?  (proba=1.00)\n",
      "9 \t 8 \t- fra \t\tInterlingua  \t\tAugmente le son.  (proba=0.67)\n",
      "10 \t 9 \t- hun \t\tHungarian    \t\tOlyan keményen dolgoztam, amennyire csak lehetséges volt.  (proba=1.00)\n",
      "11 \t 10 \t- eng \t\tEnglish      \t\tHas it been proven that there's a link between tobacco and lung cancer?  (proba=1.00)\n",
      "12 \t 11 \t- fra \t\tFrench       \t\tVous devriez vous remarier.  (proba=1.00)\n",
      "13 \t 12 \t- rus \t\tRussian      \t\tДруг друга твоего отца - не обязательно друг твоего отца.  (proba=1.00)\n",
      "14 \t 13 \t- hun \t\tHungarian    \t\tEzt a bájos, orosz teremtést az Ermitázsban ismertem meg.  (proba=1.00)\n",
      "15 \t 14 \t- nld \t\tDutch        \t\tHet Engels, Russisch, Spaans en Hindi stammen af ​​van een gemeenschappelijke vooroudertaal.  (proba=1.00)\n",
      "16 \t 15 \t- tur \t\tTurkish      \t\tBaşka bir randevum var.  (proba=1.00)\n",
      "17 \t 16 \t- kab \t\tKabyle       \t\tTjerḥem-asent-tt.  (proba=0.87)\n",
      "18 \t 17 \t- epo \t\tEsperanto    \t\tGekuzoj kultivas teplantojn en sia bieno.  (proba=1.00)\n",
      "19 \t 18 \t- mai \t\tHindi        \t\tकेना छी अहाँ।  (proba=0.97)\n",
      "20 \t 19 \t- aze \t\tTurkish      \t\tOnun ana dili rus dilidir.  (proba=0.99)\n",
      "21 \t 20 \t- fin \t\tFinnish      \t\tMitä on polyamoria?  (proba=1.00)\n",
      "22 \t 21 \t- jpn \t\tJapanese     \t\tライオンはその大きな口を開けて吠えた。  (proba=1.00)\n",
      "23 \t 22 \t- rus \t\tRussian      \t\tЭто автопортрет.  (proba=1.00)\n",
      "24 \t 23 \t- ber \t\tBerber langu \t\tTom yebɣa ad yessulli.  (proba=0.98)\n",
      "25 \t 24 \t- tur \t\tTurkish      \t\tTom romantik.  (proba=0.70)\n",
      "26 \t 25 \t- kab \t\tKabyle       \t\tAmek dɣa ara kettbent azemz s tlatinit?  (proba=1.00)\n",
      "27 \t 26 \t- tur \t\tTurkish      \t\tPeşimizde kimin olduğunu söyle bize.  (proba=1.00)\n",
      "28 \t 27 \t- por \t\tPortuguese   \t\tEu jamais faria isso por elas.  (proba=1.00)\n",
      "29 \t 28 \t- hun \t\tHungarian    \t\tVan valami, amit el akarok neked mondani.  (proba=1.00)\n",
      "30 \t 29 \t- ita \t\tItalian      \t\tDice di non essere interessata.  (proba=1.00)\n"
     ]
    }
   ],
   "source": [
    "n_bad_max = 30\n",
    "n_bad = 0\n",
    "print(\"\\tN°Ligne\\tL. réelle\\tPréd. Naive B.\\t\\tPhrase\")\n",
    "for i in range(len(df)):\n",
    "    if (n_bad<n_bad_max):\n",
    "        if (df['lan_code'].iloc[i] != lang_id_nb(df['sentence'].iloc[i])):\n",
    "            n_bad +=1\n",
    "            print(n_bad,'\\t',i,'\\t-',df['lan_code'].iloc[i],'\\t\\t'+lang_id_nb(df['sentence'].iloc[i]).ljust(12)[:12],'\\t\\t'+\n",
    "                  df['sentence'].iloc[i],\" (proba={:.2f}\".format(max(clf_nb.predict_proba(create_BOW([df['sentence'].iloc[i]]))[0]))+\")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
