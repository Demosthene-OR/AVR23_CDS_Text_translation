{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a996d864-6a89-4513-95ba-2edb457d25be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Utilisation simple des identifieurs de langues** (rev2) **avec les :**\n",
    ">### **- '*Sparse*' Bag Of Words**\n",
    ">### **- Tokenisations BERT ou Tiktoken**\n",
    ">### **- CountVectorizer utilisant une tokenisation '*custom*'**\n",
    ">### **- Classificateurs Naïve Bayes et Gradiant Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e67500-a4c1-4d58-8f4a-1be20434be6d",
   "metadata": {},
   "source": [
    "#### **Choix du tokenizer** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb14523-04a8-424c-976a-d61585c0bbf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Choix de la Tokenisation (False = BERT, True Tiktoken)\n",
    "titoken_tokenization = True\n",
    "\n",
    "# Ce parametre permet éventuellement d'équilibrer de nombre de phrase par langue.\n",
    "# Si ce parametre est très grand, tout le corpus sera lu. \n",
    "nb_phrase_lang = 500000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bad4ba-8678-41a4-9008-ab1915eddae6",
   "metadata": {},
   "source": [
    "#### **Lectures des phrases de \"sentences.csv\", et de leur étiquette \"Langue\" pour les langues sélectionnées**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "491d66b2-90bd-49e9-99cb-9601edf52a57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes de sentence.csv: 1750000\n",
      "Nombre de phrases par langue  ['eng', 'fra', 'deu', 'spa', 'ita'] : [350000, 350000, 350000, 350000, 350000]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng</td>\n",
       "      <td>She is afraid of death.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ita</td>\n",
       "      <td>Indovina cosa scelgo io.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spa</td>\n",
       "      <td>¿Puedo ayudarlo? \"No, gracias. Solo estoy mira...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ita</td>\n",
       "      <td>Io non sono una fricchettona!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deu</td>\n",
       "      <td>Es sind schon fast 10 Jahre vergangen, aber du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spa</td>\n",
       "      <td>Creía que me quería.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eng</td>\n",
       "      <td>This school sets high moral standards for pupils.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eng</td>\n",
       "      <td>Man is judged by his courage, woman by her charm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fra</td>\n",
       "      <td>Je mange des pruneaux sucrés.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fra</td>\n",
       "      <td>J'ai écrit une chanson pour toi.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lan_code                                           sentence\n",
       "0      eng                            She is afraid of death.\n",
       "1      ita                           Indovina cosa scelgo io.\n",
       "2      spa  ¿Puedo ayudarlo? \"No, gracias. Solo estoy mira...\n",
       "3      ita                      Io non sono una fricchettona!\n",
       "4      deu  Es sind schon fast 10 Jahre vergangen, aber du...\n",
       "5      spa                               Creía que me quería.\n",
       "6      eng  This school sets high moral standards for pupils.\n",
       "7      eng  Man is judged by his courage, woman by her charm.\n",
       "8      fra                      Je mange des pruneaux sucrés.\n",
       "9      fra                   J'ai écrit une chanson pour toi."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1749990</th>\n",
       "      <td>deu</td>\n",
       "      <td>Es geschieht heutzutage ja so viel in unserer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749991</th>\n",
       "      <td>spa</td>\n",
       "      <td>El almuerzo está preparado.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749992</th>\n",
       "      <td>eng</td>\n",
       "      <td>I've seen enough.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749993</th>\n",
       "      <td>ita</td>\n",
       "      <td>Hanno accelerato il passo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749994</th>\n",
       "      <td>fra</td>\n",
       "      <td>Elle en pince pour ce garçon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749995</th>\n",
       "      <td>deu</td>\n",
       "      <td>Wer von uns wünschte nicht manchmal, dass er d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749996</th>\n",
       "      <td>ita</td>\n",
       "      <td>No! Io odio i broccoli!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749997</th>\n",
       "      <td>fra</td>\n",
       "      <td>Tu seras tuée !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749998</th>\n",
       "      <td>fra</td>\n",
       "      <td>Tom aurait dû manger plus.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749999</th>\n",
       "      <td>eng</td>\n",
       "      <td>He took the video to a local TV station.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lan_code                                           sentence\n",
       "1749990      deu  Es geschieht heutzutage ja so viel in unserer ...\n",
       "1749991      spa                        El almuerzo está preparado.\n",
       "1749992      eng                                  I've seen enough.\n",
       "1749993      ita                         Hanno accelerato il passo.\n",
       "1749994      fra                      Elle en pince pour ce garçon.\n",
       "1749995      deu  Wer von uns wünschte nicht manchmal, dass er d...\n",
       "1749996      ita                            No! Io odio i broccoli!\n",
       "1749997      fra                                    Tu seras tuée !\n",
       "1749998      fra                         Tom aurait dû manger plus.\n",
       "1749999      eng           He took the video to a local TV station."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ouvrir le fichier d'entrée en mode lecture\n",
    "def create_lang_df(path):\n",
    "    df = pd.read_csv(path, index_col ='id')\n",
    "    return df\n",
    "\n",
    "df_big = create_lang_df('../data/multilingue/sentences.csv')\n",
    "lan_code = ['eng','fra','deu','spa','ita']\n",
    "df = pd.DataFrame(columns=df_big.columns)\n",
    "for i in range(len(lan_code)):\n",
    "    df= pd.concat([df, df_big[df_big['lan_code']==lan_code[i]].iloc[:nb_phrase_lang]])\n",
    "df = df.sample(frac=1, random_state=3).reset_index(drop=True)\n",
    "n_rows = len(df)\n",
    "print('Nombre de lignes de sentence.csv:',n_rows)\n",
    "nb_phrases_lang =[]\n",
    "for l in lan_code:\n",
    "    nb_phrases_lang.append(sum(df['lan_code']==l))\n",
    "print(\"Nombre de phrases par langue \",lan_code,\":\",nb_phrases_lang)\n",
    "display(df.head(10))\n",
    "display(df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78bb348-f3c0-4455-b39e-a0b26527556a",
   "metadata": {},
   "source": [
    "#### **Selection du tokenizer** en fonction de la variable titoken_tokenization <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce260244-a177-43b5-accf-0564548c2fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selection du tokenizer\n",
    "if titoken_tokenization:\n",
    "    import tiktoken\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "else:\n",
    "    from transformers import BertTokenizerFast\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb9b00-6cc3-47bc-afcf-b00fdef8738b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Préparation de la vectorisation par CountVectorizer** <font color='red'>(nécéssaire pour traduction texte libre)</font>\n",
    "#### **Création de la fonction de création d'un Bags  Of Worlds pour les phrases à identifier** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df235a48-3ea5-47a7-af0a-f91fd20af84e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Les 2 fonctions suivantes sont nécéssaires afin de sérialiser ces parametre de CountVectorizer\n",
    "# et ainsi de sauvegarder le vectorizer pour un un usage ultérieur sans utiliser X_train pour  le réinitialiser\n",
    "def custom_tokenizer(text):\n",
    "    tokens = tokenizer.encode(text)  # Cela divise le texte en mots\n",
    "    return tokens\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    return text\n",
    "\n",
    "# CountVectorizer a une liste de phrase en entrée.\n",
    "# Cette fonction met les données d'entrée dans le bon format\n",
    "def format_to_vectorize(data):\n",
    "    X_tok = []\n",
    "    if \"DataFrame\" in str(type(data)):sentences = df.tolist()\n",
    "    elif \"str\" in str(type(data)):\n",
    "        sentences =[data]\n",
    "    else: sentences = data\n",
    "                          \n",
    "    for sentence in sentences:\n",
    "        X_tok.append(sentence) # ('¤'.join([tokenizer.decode([ids]) for ids in tokenizer.encode(sentence)])+'¤')\n",
    "    return X_tok\n",
    "\n",
    "def create_BOW(data):\n",
    "    global vectorizer\n",
    "    \n",
    "    X_tok = format_to_vectorize(data)\n",
    "    X = vectorizer.transform(X_tok)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf34d21-5bde-4d0f-a149-0ad5a48a39ed",
   "metadata": {},
   "source": [
    "#### **Chargement du vectorizer** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "773d2983-dd0a-450a-af97-a5d3034fe315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectorizer():\n",
    "    global dict_token, dict_ids, nb_token\n",
    "    \n",
    "    if titoken_tokenization: path = '../data/vectorizer_tiktoken.pkl'\n",
    "    else: path = '../data/vectorizer_BERT.pkl'\n",
    "    vectorizer = joblib.load(path)\n",
    "    dict_token = {tokenizer.decode([cle]): cle for cle, valeur in vectorizer.vocabulary_.items()}\n",
    "    dict_ids = {cle: tokenizer.decode([cle]) for cle, valeur in vectorizer.vocabulary_.items()} #dict_ids.items()}\n",
    "    nb_token = len(vectorizer.vocabulary_)\n",
    "    return vectorizer\n",
    "\n",
    "vectorizer = load_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a96239-98ab-47b0-acf9-1660e16fc9ab",
   "metadata": {},
   "source": [
    "#### **Choix du nom du fichier du classifieur sauvegardé** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc778b1-231d-42c6-9218-144ee7e88da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_file_name(titoken_tokenization, classifier):\n",
    "    if titoken_tokenization:\n",
    "        return \"id_lang_tiktoken_\"+classifier+\"_sparse.pkl\"\n",
    "    else:\n",
    "        return \"id_lang_BERT_\"+classifier+\"_sparse.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923233dd-f03a-400a-bc7c-b5fc4914ca8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Chargement du classificateur entrainé avec l'algorithme Naïve Bayes** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54b3449f-f635-402a-b3bf-5c163fadad52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "# Chargement du classificateur sauvé\n",
    "clf_nb = joblib.load(\"../data/\"+get_file_name(titoken_tokenization,\"nb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95249cde-cfc5-487c-b579-c590ee84ca42",
   "metadata": {},
   "source": [
    "#### **Chargement du classificateur entrainé avec l'algorithme Gradiant Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbfa6b8c-172e-4cb7-ab10-20ed7154c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Chargement du classificateur sauvé\n",
    "clf_gb = joblib.load(\"../data/\"+get_file_name(titoken_tokenization,\"gb\"))\n",
    "######### dict_ids, decoded_keys = load_dict_token() ######### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de709b0-83c2-4b3f-a018-ea6c10531f99",
   "metadata": {},
   "source": [
    "#### **Definition de fonctions identificateur de langue avec Naive Bayes** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753c88f7-991f-4097-af13-6bae4f1e4090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Chargement du classificateur sauvegardé\n",
    "clf_nb = joblib.load(\"../data/\"+get_file_name(titoken_tokenization,\"nb\"))\n",
    "vectorizer = load_vectorizer()\n",
    "\n",
    "# Lisez le contenu du fichier JSON\n",
    "with open('../data/multilingue/lan_to_language.json', 'r') as fichier:\n",
    "    lan_to_language = json.load(fichier)\n",
    "\n",
    "\n",
    "\n",
    "def lang_id_nb(sentences):\n",
    "    if \"str\" in str(type(sentences)):\n",
    "        return lan_to_language[clf_nb.predict(create_BOW(sentences))[0]]\n",
    "    else: return [lan_to_language[l] for l in clf_nb.predict(create_BOW(sentences))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cdd3d07-abf0-48e3-9801-6cb2e4263d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lang_id_gb(sentences):\n",
    "    if \"str\" in str(type(sentences)):\n",
    "        return lan_to_language[clf_gb.predict(create_BOW(sentences))[0]]\n",
    "    else:\n",
    "        return [lan_to_language[l] for l in clf_gb.predict(create_BOW(sentences))]        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82b0a7-3649-4c66-97d2-e38c52e2fbfc",
   "metadata": {},
   "source": [
    "#### **Exemples d'utilisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a73abd8-94fb-4ed5-8ad9-3308c95ca077",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\t lang\t Phrase\n",
      "0 -\t eng \t france is often snowy during spring , and it is relaxing in january .\n",
      "1 -\t fra \t elle adore les voitures très luxueuses, et toi ?\n",
      "2 -\t eng \t she loves very luxurious cars, don't you?\n",
      "3 -\t spa \t vamos a la playa\n",
      "4 -\t deu \t Ich heiße Keyne, und das ist wunderbar\n",
      "5 -\t e,f,d \t she loves you, mais elle te hait aussi, and das ist traurig\n",
      "6 -\t en \t I ate caviar\n",
      "7 -\t ita \t Vogliamo visitare il Colosseo e nuotare nel Tevere.\n",
      "8 -\t fra \t Il ne me plaît pas, mais elle, elle me plaît.\n",
      "9 -\t eng \t Mr Sato ran a supermarket in his hometown before he came to Tokyo.\n",
      "10 -\t deu \t Er erwarb die amerikanische Staatsbürgerschaft.\n",
      "11 -\t spa \t Yo ocupo todo el tiempo que puedo en leer.\n",
      "12 -\t eng \t His life rests on her.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Instanciation d'exemples\n",
    "\n",
    "sentence_no = random.sample(range(len(df)),5)\n",
    "\n",
    "exemples = [\"france is often snowy during spring , and it is relaxing in january .\",\n",
    "           \"elle adore les voitures très luxueuses, et toi ?\",\n",
    "           \"she loves very luxurious cars, don't you?\",\n",
    "           \"vamos a la playa\",\n",
    "           \"Ich heiße Keyne, und das ist wunderbar\",\n",
    "           \"she loves you, mais elle te hait aussi, and das ist traurig\", # Attention à cette phrase trilingue\n",
    "           \"I ate caviar\", \n",
    "           \"Vogliamo visitare il Colosseo e nuotare nel Tevere.\",\n",
    "            df['sentence'].iloc[sentence_no[0]],\n",
    "            df['sentence'].iloc[sentence_no[1]],\n",
    "            df['sentence'].iloc[sentence_no[2]],\n",
    "            df['sentence'].iloc[sentence_no[3]],\n",
    "            df['sentence'].iloc[sentence_no[4]],\n",
    "          ]\n",
    "lang_exemples = ['eng','fra','eng','spa','deu','e,f,d','en','ita',df['lan_code'].iloc[sentence_no[0]],df['lan_code'].iloc[sentence_no[1]],df['lan_code'].iloc[sentence_no[2]],\n",
    "                 df['lan_code'].iloc[sentence_no[3]],df['lan_code'].iloc[sentence_no[4]]]\n",
    "print('no\\t lang\\t Phrase')                            \n",
    "for i in range(len(exemples)):\n",
    "    print(i,'-\\t',lang_exemples[i],'\\t',exemples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5b4a279-9aea-494e-b468-de0f024cd899",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langue réelle\tPréd. Naive B.\tPréd. Grad. B.\tPhrase\n",
      "eng\t\tEnglish\t\tEnglish\t\tfrance is often snowy during spring , and it is relaxing in january .\n",
      "fra\t\tFrench\t\tFrench\t\telle adore les voitures très luxueuses, et toi ?\n",
      "eng\t\tEnglish\t\tEnglish\t\tshe loves very luxurious cars, don't you?\n",
      "spa\t\tSpanish\t\tSpanish\t\tvamos a la playa\n",
      "deu\t\tGerman\t\tGerman\t\tIch heiße Keyne, und das ist wunderbar\n",
      "e,f,d\t\tFrench\t\tGerman\t\tshe loves you, mais elle te hait aussi, and das ist traurig\n",
      "en\t\tItalian\t\tEnglish\t\tI ate caviar\n",
      "ita\t\tItalian\t\tItalian\t\tVogliamo visitare il Colosseo e nuotare nel Tevere.\n",
      "fra\t\tFrench\t\tFrench\t\tIl ne me plaît pas, mais elle, elle me plaît.\n",
      "eng\t\tEnglish\t\tEnglish\t\tMr Sato ran a supermarket in his hometown before he came to Tokyo.\n",
      "deu\t\tGerman\t\tGerman\t\tEr erwarb die amerikanische Staatsbürgerschaft.\n",
      "spa\t\tSpanish\t\tSpanish\t\tYo ocupo todo el tiempo que puedo en leer.\n",
      "eng\t\tEnglish\t\tEnglish\t\tHis life rests on her.\n"
     ]
    }
   ],
   "source": [
    "# Affichage des prédictions\n",
    "print(\"Langue réelle\\tPréd. Naive B.\\tPréd. Grad. B.\\tPhrase\")\n",
    "for i in range(len(exemples)):\n",
    "    print(lang_exemples[i]+'\\t\\t'+lang_id_nb(exemples[i])+'\\t\\t'+lang_id_gb(exemples[i])+'\\t\\t'+exemples[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8d1c1-e5dd-4a17-8aee-1305fbf4c45a",
   "metadata": {},
   "source": [
    "> **Recherche des phrases mal classées par Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d458174a-c2e1-41e8-a84a-35584eaac751",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN°Ligne\tL. réelle\tPréd. Naive B.\t\tPhrase\n",
      "1 \t 0 \t- eng \t\tEnglish      \t\tShe is afraid of death.  (proba=1.00)\n",
      "2 \t 1 \t- ita \t\tItalian      \t\tIndovina cosa scelgo io.  (proba=1.00)\n",
      "3 \t 2 \t- spa \t\tSpanish      \t\t¿Puedo ayudarlo? \"No, gracias. Solo estoy mirando.\"  (proba=1.00)\n",
      "4 \t 3 \t- ita \t\tItalian      \t\tIo non sono una fricchettona!  (proba=1.00)\n",
      "5 \t 4 \t- deu \t\tGerman       \t\tEs sind schon fast 10 Jahre vergangen, aber du bist unverändert schön.  (proba=1.00)\n",
      "6 \t 5 \t- spa \t\tSpanish      \t\tCreía que me quería.  (proba=1.00)\n",
      "7 \t 6 \t- eng \t\tEnglish      \t\tThis school sets high moral standards for pupils.  (proba=1.00)\n",
      "8 \t 7 \t- eng \t\tEnglish      \t\tMan is judged by his courage, woman by her charm.  (proba=1.00)\n",
      "9 \t 8 \t- fra \t\tFrench       \t\tJe mange des pruneaux sucrés.  (proba=1.00)\n",
      "10 \t 9 \t- fra \t\tFrench       \t\tJ'ai écrit une chanson pour toi.  (proba=1.00)\n",
      "11 \t 10 \t- fra \t\tFrench       \t\tTom avait déjà rejoint son domicile.  (proba=1.00)\n",
      "12 \t 11 \t- ita \t\tItalian      \t\tTom è una spia.  (proba=1.00)\n",
      "13 \t 12 \t- ita \t\tItalian      \t\tVoi dovete essere preparate.  (proba=1.00)\n",
      "14 \t 13 \t- spa \t\tSpanish      \t\tSi tan solo el día tuviera 48 horas...  (proba=1.00)\n",
      "15 \t 14 \t- fra \t\tFrench       \t\tVous êtes une femme d'une grande patience.  (proba=1.00)\n",
      "16 \t 15 \t- fra \t\tFrench       \t\tVous n'êtes ni bon ni mauvais.  (proba=1.00)\n",
      "17 \t 16 \t- spa \t\tSpanish      \t\tEl sake se hace con arroz.  (proba=1.00)\n",
      "18 \t 17 \t- spa \t\tSpanish      \t\tEn realidad Tom ya no es estudiante.  (proba=1.00)\n",
      "19 \t 18 \t- deu \t\tGerman       \t\tSie lässt für ihre Arbeit keine Begeisterung erkennen.  (proba=1.00)\n",
      "20 \t 19 \t- eng \t\tEnglish      \t\tCan someone who does not know the meaning of black really talk about what white is?  (proba=1.00)\n",
      "21 \t 20 \t- spa \t\tSpanish      \t\tLa carne estaba zapatera.  (proba=1.00)\n",
      "22 \t 21 \t- fra \t\tFrench       \t\tLe monosyllabe a une étrange capacité d'immensité : mer, nuit, jour, bien, mal, mort, oui, non, dieu.  (proba=1.00)\n",
      "23 \t 22 \t- deu \t\tGerman       \t\tDas Volk fordert: „Alle Diebe sollen in Kammern sitzen!“ – Der Staatsanwalt fragt: „Wo eigentlich – in den Abgeordneten- oder Gefängniskammern?“  (proba=1.00)\n",
      "24 \t 23 \t- eng \t\tEnglish      \t\tTom really enjoyed his meal.  (proba=1.00)\n",
      "25 \t 24 \t- eng \t\tEnglish      \t\tDo you want me to watch Tom for you?  (proba=1.00)\n",
      "26 \t 25 \t- spa \t\tSpanish      \t\tTom y Mary se conocen muy bien.  (proba=1.00)\n",
      "27 \t 26 \t- spa \t\tSpanish      \t\tTe busqué por todas partes y no te encontré.  (proba=1.00)\n",
      "28 \t 27 \t- fra \t\tFrench       \t\tLe pain est sur la table.  (proba=1.00)\n",
      "29 \t 28 \t- ita \t\tItalian      \t\tDovreste conoscerla.  (proba=1.00)\n",
      "30 \t 29 \t- deu \t\tGerman       \t\tMorgen ist Samstag, der 5. Februar 2011.  (proba=1.00)\n"
     ]
    }
   ],
   "source": [
    "n_bad_max = 30\n",
    "n_bad = 0\n",
    "print(\"\\tN°Ligne\\tL. réelle\\tPréd. Naive B.\\t\\tPhrase\")\n",
    "for i in range(len(df)):\n",
    "    if (n_bad<n_bad_max):\n",
    "        if (df['lan_code'].iloc[i] != lang_id_nb(df['sentence'].iloc[i])):\n",
    "            n_bad +=1\n",
    "            print(n_bad,'\\t',i,'\\t-',df['lan_code'].iloc[i],'\\t\\t'+lang_id_nb(df['sentence'].iloc[i]).ljust(12),'\\t\\t'+\n",
    "                  df['sentence'].iloc[i],\" (proba={:.2f}\".format(max(clf_nb.predict_proba(create_BOW([df['sentence'].iloc[i]]))[0]))+\")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
