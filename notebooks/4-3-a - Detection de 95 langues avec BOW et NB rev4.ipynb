{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9a2a87-8295-469d-b2c7-f22cb3dbf287",
   "metadata": {},
   "source": [
    "<h1><bold>\n",
    "<hr style=\"border-width:2px;border-color:#1664c8\">\n",
    "Partie <b>4</b><br>\n",
    "    <center>Identification de langue parmi 95<br></center>\n",
    "<div style=\"text-align: right;\">Version 4</div><br>\n",
    "<hr style=\"border-width:2px;border-color:#1664c8\">\n",
    "</h1></bold>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996d864-6a89-4513-95ba-2edb457d25be",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Identification de langue avec :**\n",
    ">### **- des '*Sparse*' Bag Of Words**\n",
    ">### **- une Tokenisations Tiktoken**\n",
    ">### **- CountVectorizer utilisant une tokenisation '*custom*'**\n",
    ">### **- un Classificateurs Naïve Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e67500-a4c1-4d58-8f4a-1be20434be6d",
   "metadata": {},
   "source": [
    "## **1 - Contruction des classificateurs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ef517-3030-4c4c-904f-0db3d49140df",
   "metadata": {},
   "source": [
    "#### **Chargement des biblothèques nécéssaires** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb14523-04a8-424c-976a-d61585c0bbf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "import random\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Ce parametre permet éventuellement d'équilibrer de nombre de phrase par langue.\n",
    "# Si ce parametre est très grand, tout le corpus sera lu. \n",
    "nb_phrase_lang = 10000000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bad4ba-8678-41a4-9008-ab1915eddae6",
   "metadata": {},
   "source": [
    "#### **Lectures des phrases de \"sentences-big.csv\", et de leur étiquette \"Langue\" pour les langues sélectionnées**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "491d66b2-90bd-49e9-99cb-9601edf52a57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes de sentence.csv: 10345978\n",
      "Nombre de langues à classer: 404\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ber</td>\n",
       "      <td>Yeqber uɛebbuḍ-iw seg wayen ččiɣ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ita</td>\n",
       "      <td>Sono venute alla moschea.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heb</td>\n",
       "      <td>אני לא יכול לפענח מה קרה.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rus</td>\n",
       "      <td>У меня очень много дел.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ita</td>\n",
       "      <td>Andiamo a imparare qualcosa in Uganda.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345973</th>\n",
       "      <td>deu</td>\n",
       "      <td>Wir werden das Problem nicht aufgreifen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345974</th>\n",
       "      <td>fra</td>\n",
       "      <td>Je suis cuit !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345975</th>\n",
       "      <td>epo</td>\n",
       "      <td>Atomenergio estas sekura.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345976</th>\n",
       "      <td>tok</td>\n",
       "      <td>o pana ala e moku tawa soweli tomo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345977</th>\n",
       "      <td>hun</td>\n",
       "      <td>Máris unod magad?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10345978 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lan_code                                  sentence\n",
       "0             ber         Yeqber uɛebbuḍ-iw seg wayen ččiɣ.\n",
       "1             ita                 Sono venute alla moschea.\n",
       "2             heb                 אני לא יכול לפענח מה קרה.\n",
       "3             rus                   У меня очень много дел.\n",
       "4             ita    Andiamo a imparare qualcosa in Uganda.\n",
       "...           ...                                       ...\n",
       "10345973      deu  Wir werden das Problem nicht aufgreifen.\n",
       "10345974      fra                            Je suis cuit !\n",
       "10345975      epo                 Atomenergio estas sekura.\n",
       "10345976      tok       o pana ala e moku tawa soweli tomo.\n",
       "10345977      hun                         Máris unod magad?\n",
       "\n",
       "[10345978 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ouvrir le fichier d'entrée en mode lecture\n",
    "def create_lang_df(path):\n",
    "    df = pd.read_csv(path, index_col ='id')\n",
    "    return df\n",
    "\n",
    "df = create_lang_df('../data/multilingue/sentences-big.csv')\n",
    "lan_code = list(set(df['lan_code']))\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "n_rows = len(df)\n",
    "print('Nombre de lignes de sentence.csv:',n_rows)\n",
    "print('Nombre de langues à classer:',len(lan_code))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71dc774-4916-4658-b015-01594a8d003b",
   "metadata": {},
   "source": [
    "#### **Réalisation d'un jeu de données d'entrainement et de test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbac26f-2f26-4e3e-9685-1aa5914569b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lan_code</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deu</td>\n",
       "      <td>Wie die Erde entstanden ist, das ist eine Frage, die sich allen stellt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deu</td>\n",
       "      <td>Drinnen ist etwas Lebendiges.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rus</td>\n",
       "      <td>Я не хотела замуж.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kab</td>\n",
       "      <td>Ssarden-ak-t.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>por</td>\n",
       "      <td>Estou piscando para ele, mas ele não está olhando.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9828674</th>\n",
       "      <td>tat</td>\n",
       "      <td>Йөрәген бүләк иткәннәр, кан әйләнешендә кыенлыклар барлыкка килсә, гаҗәпләнергә тиеш түгел.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9828675</th>\n",
       "      <td>ber</td>\n",
       "      <td>Cmumeḥ-d yid-i ma ulac aɣilif.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9828676</th>\n",
       "      <td>kab</td>\n",
       "      <td>Ceɛɛel-as ɣer Tasga Mellul.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9828677</th>\n",
       "      <td>eng</td>\n",
       "      <td>Tom can't stand vegetables.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9828678</th>\n",
       "      <td>heb</td>\n",
       "      <td>היא עושה מדיטציה.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9757778 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lan_code  \\\n",
       "0            deu   \n",
       "1            deu   \n",
       "2            rus   \n",
       "3            kab   \n",
       "4            por   \n",
       "...          ...   \n",
       "9828674      tat   \n",
       "9828675      ber   \n",
       "9828676      kab   \n",
       "9828677      eng   \n",
       "9828678      heb   \n",
       "\n",
       "                                                                                            sentence  \n",
       "0                            Wie die Erde entstanden ist, das ist eine Frage, die sich allen stellt.  \n",
       "1                                                                      Drinnen ist etwas Lebendiges.  \n",
       "2                                                                                 Я не хотела замуж.  \n",
       "3                                                                                      Ssarden-ak-t.  \n",
       "4                                                 Estou piscando para ele, mas ele não está olhando.  \n",
       "...                                                                                              ...  \n",
       "9828674  Йөрәген бүләк иткәннәр, кан әйләнешендә кыенлыклар барлыкка килсә, гаҗәпләнергә тиеш түгел.  \n",
       "9828675                                                               Cmumeḥ-d yid-i ma ulac aɣilif.  \n",
       "9828676                                                                  Ceɛɛel-as ɣer Tasga Mellul.  \n",
       "9828677                                                                  Tom can't stand vegetables.  \n",
       "9828678                                                                            היא עושה מדיטציה.  \n",
       "\n",
       "[9757778 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes par langue:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_phrases_lang</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lan_code</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>afr</th>\n",
       "      <td>4137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ara</th>\n",
       "      <td>38651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arq</th>\n",
       "      <td>2336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asm</th>\n",
       "      <td>3205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avk</th>\n",
       "      <td>4102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>war</th>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wuu</th>\n",
       "      <td>4757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yid</th>\n",
       "      <td>9632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yue</th>\n",
       "      <td>6230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsm</th>\n",
       "      <td>6610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          nb_phrases_lang\n",
       "lan_code                 \n",
       "afr                  4137\n",
       "ara                 38651\n",
       "arq                  2336\n",
       "asm                  3205\n",
       "avk                  4102\n",
       "...                   ...\n",
       "war                  2025\n",
       "wuu                  4757\n",
       "yid                  9632\n",
       "yue                  6230\n",
       "zsm                  6610\n",
       "\n",
       "[95 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# créer 2 dataframes: 1 train (95% des phrases) et 1 test (5% des phrases)\n",
    "n_train = int(n_rows*0.95)\n",
    "df_train = df.iloc[:n_train].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_test = df.iloc[n_train:].sample(frac=1, random_state=24).reset_index(drop=True)\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "df_lan = pd.DataFrame(data= df.groupby('lan_code').size(), columns = ['nb_phrases_lang'] )\n",
    "\n",
    "# Filtrage des langues qui ont peu de phrases (>2000)\n",
    "df_lan = df_lan.loc[df_lan['nb_phrases_lang']>=2000]\n",
    "list_lan = list(set(df_lan.index))\n",
    "df_train = df_train[df_train['lan_code'].isin(list_lan)]\n",
    "df_test = df_test[df_test['lan_code'].isin(list_lan)]\n",
    "print('df_train:')\n",
    "display(df_train)\n",
    "print('Nombre de lignes par langue:')\n",
    "display(df_lan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ccdf5-a5c4-4181-8b5a-22a3148d331a",
   "metadata": {},
   "source": [
    "#### **Préparation de la vectorisation par CountVectorizer** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be0a0e5-ac97-4d8a-93fb-bc4785e90808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selection du tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Les 2 fonctions suivantes sont nécéssaires afin de sérialiser ces parametre de CountVectorizer\n",
    "# et ainsi de sauvegarder le vectorizer pour un un usage ultérieur sans utiliser X_train pour  le réinitialiser\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    tokens = tokenizer.encode(text)  # Cela divise le texte en mots\n",
    "    return tokens\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    return text\n",
    "\n",
    "# CountVectorizer a une liste de phrase en entrée.\n",
    "# Cette fonction met les données d'entrée dans le bon format\n",
    "def format_to_vectorize(data):\n",
    "    X_tok = []\n",
    "    if \"DataFrame\" in str(type(data)):sentences = data.tolist()\n",
    "    elif \"str\" in str(type(data)):\n",
    "        sentences =[data]\n",
    "    else: sentences = data\n",
    "                          \n",
    "    for sentence in sentences:\n",
    "        X_tok.append(sentence) # ('¤'.join([tokenizer.decode([ids]) for ids in tokenizer.encode(sentence)])+'¤')\n",
    "    return X_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adba3da-befd-44cd-a602-13b87cb664e6",
   "metadata": {},
   "source": [
    "#### **Création de la fonction Vectorizer et de la fonction de création d'un Bags  Of Worlds** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "029f861d-e36b-47e0-a43d-664e6f00720c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Création d'un vectorizer et du sparse BOW (X_train) avec le nombre d'apparitions\n",
    "global vectorizer, dict_ids, dict_token\n",
    "\n",
    "def create_vectorizer(X_train_tok):\n",
    "    global vectorizer, dict_ids, dict_token\n",
    "    \n",
    "    # token_pattern = r\"[a-zA-Z0-9\\s\\.\\,\\?\\:\\;]+\" \n",
    "    # vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=lambda x: tokenizer.encode(x), preprocessor=lambda x: x) #,token_pattern=token_pattern\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=custom_tokenizer, preprocessor=custom_preprocessor) #,token_pattern=token_pattern\n",
    "    vectorizer.fit(X_train_tok)\n",
    "    \n",
    "    # Création de dictionnaire des Token et des ids \n",
    "    dict_token = {tokenizer.decode([cle]): cle for cle, valeur in vectorizer.vocabulary_.items()}\n",
    "    dict_ids = {cle: tokenizer.decode([cle]) for cle, valeur in vectorizer.vocabulary_.items()} #dict_ids.items()}\n",
    "    return \n",
    "\n",
    "def create_BOW(data, vectorizer_to_create=False):\n",
    "    global vectorizer\n",
    "    \n",
    "    X_tok = format_to_vectorize(data)\n",
    "    if vectorizer_to_create:\n",
    "        create_vectorizer(X_tok)\n",
    "    X = vectorizer.transform(X_tok)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de78ee6-5749-4fe2-bbb4-6ae330e6415a",
   "metadata": {},
   "source": [
    "#### **Création du BOW Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88502575-7222-4b4e-a615-e69b882830a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = create_BOW(df_train['sentence'], True)\n",
    "y_train = df_train['lan_code'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4988d-49c4-43d1-8219-4971eea7d0ad",
   "metadata": {},
   "source": [
    "#### **Création du BOW Test**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2e470a4-55c9-4516-8136-1331a5984720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = create_BOW(df_test['sentence'])\n",
    "y_test = df_test['lan_code'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07204db8-0bd2-4742-b518-c12e5d28afe6",
   "metadata": {},
   "source": [
    "#### **Sauvegarde/Chargement du vectorizer** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04628919-c011-407a-8100-ff9bbd4b97e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definition de fonction de sauvegarde et chargement du dictionnaire des tokens utilisés\n",
    "def save_vectorizer(vectorizer):\n",
    "    path = '../data/vectorizer_tiktoken_big.pkl'\n",
    "    joblib.dump(vectorizer, path)\n",
    "\n",
    "def load_vectorizer():\n",
    "    global dict_token, dict_ids, nb_token\n",
    "    \n",
    "    path = '../data/vectorizer_tiktoken_big.pkl'\n",
    "    vectorizer = joblib.load(path)\n",
    "    dict_token = {tokenizer.decode([cle]): cle for cle, valeur in vectorizer.vocabulary_.items()}\n",
    "    dict_ids = {cle: tokenizer.decode([cle]) for cle, valeur in vectorizer.vocabulary_.items()} #dict_ids.items()}\n",
    "    nb_token = len(vectorizer.vocabulary_)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c62c934-15d2-4a7e-9300-6cb288d3fcce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_vectorizer(vectorizer)\n",
    "\n",
    "vectorizer = load_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aad8bf-4b26-411e-84be-4280d9940bd5",
   "metadata": {},
   "source": [
    "#### **Definition d'une fonction ids->colonne de X_train et col->ids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ded52ed4-1600-454a-83f6-196321e9a341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ids2col(list_ids):\n",
    "    d = dict(vectorizer.vocabulary_.items())\n",
    "    list_col = []\n",
    "    for ids in list_ids:\n",
    "        list_col.append(d[ids])\n",
    "    return list_col\n",
    "\n",
    "def col2ids(list_col):\n",
    "    d = dict(vectorizer.vocabulary_.items())\n",
    "    list_ids = []\n",
    "    for col in list_col:\n",
    "        for ids, c in d.items():\n",
    "            if col==c:\n",
    "                list_ids.append(ids)\n",
    "                break\n",
    "    return list_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a164735-49e1-4944-a92a-2c4e375945c9",
   "metadata": {},
   "source": [
    "#### **Création d'un dictionnaire des tokens avec leur fréquence d'apparition dans Train**\n",
    "#### **Définition d'une liste de token trié par fréquence d'apparition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "508009b3-b248-46b3-9f5a-0c5c93e35e00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens : 58951\n",
      "Liste des 50 tokens les plus fréquents: ['.', ',', '?', ' a', 'i', 'Tom', 'a', 'u', ' to', ' la', ' de', ' ', ' t', ' d', 'I', 'as', ' Tom', ' k', 'е', 'en', ' n', 'а', 'и', ' in', ' the', ' y', 'ו�', 'is', 'у', 'o', 'о', 'ом', ' с', 'er', ' в', 't', 'z', 'T', '!', ' that', 'ı', '。', ' i', \"'t\", 'י�', ' ad', 'in', ' ne', 'em', ' is']\n"
     ]
    }
   ],
   "source": [
    "freq = X_train.sum(axis=0)\n",
    "list_ids = col2ids(range(len(dict_ids)))\n",
    "list_token = [tokenizer.decode([ids]) for ids in list_ids]\n",
    "dict_freq = dict(zip(list_token,freq.tolist()[0]))\n",
    "dict_freq = dict(sorted(dict_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "def ids2token(ids):\n",
    "    for token, valeur in dict_ids.items():\n",
    "        if valeur == ids:\n",
    "            token_trouvee = token\n",
    "            break\n",
    "    return token_trouvee\n",
    "\n",
    "nb_token = len(vectorizer.vocabulary_)\n",
    "print(\"Nombre de tokens :\",nb_token)\n",
    "\n",
    "# Définition d'une liste 'écrite' des tokens : decoded_keys\n",
    "# decoded_keys = [tokenizer.decode([ids]) for ids in [ids2token(key) for key in list(dict_freq.keys())]]\n",
    "decoded_keys = list(dict_freq.keys())\n",
    "print(\"Liste des 50 tokens les plus fréquents:\",decoded_keys[:50])\n",
    "\n",
    "# vocab_size = max(max(row) for row in X_train) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a96239-98ab-47b0-acf9-1660e16fc9ab",
   "metadata": {},
   "source": [
    "#### **Choix du nom du fichier de sauvegarde du classifieur** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acc778b1-231d-42c6-9218-144ee7e88da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_file_name(titoken_tokenization, classifier):\n",
    "    return \"id_lang_tiktoken_\"+classifier+\"_sparse_big.pkl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923233dd-f03a-400a-bc7c-b5fc4914ca8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Création d'un classificateur avec l'algorithme Naïve Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c40a11f5-cf0d-4c5f-8ee1-29f6a05bf4df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "# On definit le classificateur Naive Bayes et on l'entraine sur les données Train:\n",
    "clf_nb = naive_bayes.MultinomialNB()  # BernoulliNB() # MultinomialNB() \n",
    "clf_nb.fit(X_train, y_train)\n",
    "\n",
    "# Save the model to a file\n",
    "# joblib.dump(clf_nb, \"../data/id_lang_tiktoken_nb_sparse_big.pkl\") ######### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54b3449f-f635-402a-b3bf-5c163fadad52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion du classificateur Naïve Bayes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Classe prédite</th>\n",
       "      <th>afr</th>\n",
       "      <th>ara</th>\n",
       "      <th>arq</th>\n",
       "      <th>asm</th>\n",
       "      <th>avk</th>\n",
       "      <th>aze</th>\n",
       "      <th>bel</th>\n",
       "      <th>ben</th>\n",
       "      <th>ber</th>\n",
       "      <th>bre</th>\n",
       "      <th>...</th>\n",
       "      <th>uig</th>\n",
       "      <th>ukr</th>\n",
       "      <th>urd</th>\n",
       "      <th>vie</th>\n",
       "      <th>vol</th>\n",
       "      <th>war</th>\n",
       "      <th>wuu</th>\n",
       "      <th>yid</th>\n",
       "      <th>yue</th>\n",
       "      <th>zsm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classe réelle</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>afr</th>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ara</th>\n",
       "      <td>0</td>\n",
       "      <td>1923</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arq</th>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avk</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>war</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wuu</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yid</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yue</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsm</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Classe prédite  afr   ara  arq  asm  avk  aze  bel  ben  ber  bre  ...  uig  \\\n",
       "Classe réelle                                                      ...        \n",
       "afr             149     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "ara               0  1923    0    0    0    0    0    0    0    0  ...    0   \n",
       "arq               0   103    0    0    0    0    0    0    2    0  ...    1   \n",
       "asm               0     0    0  110    0    0    0   63    0    0  ...    0   \n",
       "avk               0     0    0    0  180    0    0    0    3    0  ...    0   \n",
       "...             ...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "war               0     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "wuu               0     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "yid               0     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "yue               0     0    0    0    0    0    0    0    0    0  ...    0   \n",
       "zsm               0     6    0    0    0    0    0    0    0    0  ...    0   \n",
       "\n",
       "Classe prédite  ukr  urd  vie  vol  war  wuu  yid  yue  zsm  \n",
       "Classe réelle                                                \n",
       "afr               0    0    0    0    0    0    0    0    0  \n",
       "ara               0    0    0    0    0    0    0    0    0  \n",
       "arq               0    0    0    0    0    0    0    0    0  \n",
       "asm               0    0    0    0    0    0    0    0    0  \n",
       "avk               0    0    0    0    0    0    0    0    0  \n",
       "...             ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "war               0    0    0    0   73    0    0    0    0  \n",
       "wuu               0    0    0    0    0   75    0    2    0  \n",
       "yid               0    0    0    0    0    0  420    0    0  \n",
       "yue               0    0    0    0    0    0    0  208    0  \n",
       "zsm               0    0    0    0    0    0    0    0   92  \n",
       "\n",
       "[95 rows x 95 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Naïve Bayes = 0.960\n"
     ]
    }
   ],
   "source": [
    "# Chargement du classificateur sauvegardé\n",
    "#clf_nb = joblib.load(\"../data/id_lang_tiktoken_nb_sparse_big.pkl\")\n",
    "\n",
    "# Verification de l'efficacité du classificateur grace à la matrice confusion\n",
    "y_pred = clf_nb.predict(X_test)\n",
    "accuracy_naive_bayes = accuracy_score(y_test, y_pred)\n",
    "print(\"Matrice de confusion du classificateur Naïve Bayes\")\n",
    "ct = pd.crosstab(y_test,y_pred,rownames=['Classe réelle'], colnames=['Classe prédite'])\n",
    "display(ct)\n",
    "print(\"Accuracy Naïve Bayes = {:.3f}\".format(accuracy_naive_bayes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b819240-7f6a-4d97-9eca-480fe9f8b690",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Classe prédite</th>\n",
       "      <th>deu</th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>ita</th>\n",
       "      <th>spa</th>\n",
       "      <th>etc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classe réelle</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deu</th>\n",
       "      <td>29330</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng</th>\n",
       "      <td>11</td>\n",
       "      <td>78916</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fra</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>24833</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ita</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>40340</td>\n",
       "      <td>38</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spa</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>18321</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etc</th>\n",
       "      <td>85</td>\n",
       "      <td>46</td>\n",
       "      <td>102</td>\n",
       "      <td>332</td>\n",
       "      <td>447</td>\n",
       "      <td>320029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Classe prédite    deu    eng    fra    ita    spa     etc\n",
       "Classe réelle                                            \n",
       "deu             29330      4      1      1      0      39\n",
       "eng                11  78916      3      7      8     182\n",
       "fra                 8      8  24833     26     13      49\n",
       "ita                 1      2     22  40340     38     173\n",
       "spa                 1      1      4     35  18321     147\n",
       "etc                85     46    102    332    447  320029"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy pour les langues d'Europe de l'Ouest = 0.991\n"
     ]
    }
   ],
   "source": [
    "ct_weurope = pd.DataFrame(ct)\n",
    "lignes_a_sommer = ct.index.difference(['eng','deu','fra','ita','spa'])\n",
    "somme_l = ct.loc[lignes_a_sommer].sum()\n",
    "ct_weurope.loc['etc'] = somme_l\n",
    "ct_weurope = ct_weurope.drop(index = ct_weurope.index.difference(['eng','deu','fra','ita','spa','etc']))\n",
    "\n",
    "colonnes_a_sommer = ct_weurope.columns.difference(['eng','deu','fra','ita','spa'])\n",
    "somme_c = ct_weurope[colonnes_a_sommer].sum(axis=1)\n",
    "ct_weurope['etc']= somme_c\n",
    "ct_weurope = ct_weurope.drop(columns=ct_weurope.columns.difference(['eng','deu','fra','ita','spa','etc']))\n",
    "display(ct_weurope)\n",
    "accuracy_weurope = (ct_weurope['eng']['eng']+ct_weurope['deu']['deu']+ct_weurope['fra']['fra']+ct_weurope['ita']['ita']+ct_weurope['spa']['spa'])/(ct_weurope.sum().sum()-ct_weurope['etc']['etc'])\n",
    "print(\"Accuracy pour les langues d'Europe de l'Ouest = {:.3f}\".format(accuracy_weurope))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de709b0-83c2-4b3f-a018-ea6c10531f99",
   "metadata": {},
   "source": [
    "#### **Definition de fonction identificateur de langue** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83d8d36f-3f1c-49a8-8367-38158996865c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Chargement du classificateur sauvegardé\n",
    "clf_nb = joblib.load(\"../data/id_lang_tiktoken_nb_sparse_big.pkl\")\n",
    "vectorizer = load_vectorizer()\n",
    "\n",
    "# Lisez le contenu du fichier JSON\n",
    "with open('../data/multilingue/lan_to_language.json', 'r') as fichier:\n",
    "    lan_to_language = json.load(fichier)\n",
    "\n",
    "\n",
    "\n",
    "def lang_id_nb(sentences):\n",
    "    if \"str\" in str(type(sentences)):\n",
    "        return lan_to_language[clf_nb.predict(create_BOW(sentences))[0]]\n",
    "    else: return [lan_to_language[l] for l in clf_nb.predict(create_BOW(sentences))]\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82b0a7-3649-4c66-97d2-e38c52e2fbfc",
   "metadata": {},
   "source": [
    "#### **Exemples d'utilisation** <font color='red'>(nécéssaire pour traduction texte libre)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a73abd8-94fb-4ed5-8ad9-3308c95ca077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instanciation d'un exemple\n",
    "exemples = [\"Er weiß überhaupt nichts über dieses Buch.\",                                                             # Phrase 0\n",
    "            \"france is often snowy during spring , and it is relaxing in january .\",                                  # Phrase 1\n",
    "           \"elle adore les voitures très luxueuses, et toi ?\",                                                        # Phrase 2\n",
    "           \"she loves very luxurious cars, don't you?\",                                                               # Phrase 3\n",
    "           \"vamos a la playa\",                                                                                        # Phrase 4\n",
    "           \"Ich heiße Keyne, und das ist wunderbar\",                                                                  # Phrase 5\n",
    "           \"she loves you much, mais elle te hait aussi and das ist traurig.\", # Attention à cette phrase trilingue   # Phrase 6\n",
    "           \"A crane raises heavy construction materials.\",                                                            # Phrase 7\n",
    "           \"Vogliamo visitare il Colosseo e nuotare nel Tevere.\",                                                     # Phrase 8\n",
    "           \"私はそれについて全く知りません\"                                                                              # Phrase 9\n",
    "          ]\n",
    "lang_exemples = ['deu','eng','fra','eng','spa','deu','en,fr,de','en','ita','jpn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5b4a279-9aea-494e-b468-de0f024cd899",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langue réelle                 : ['deu', 'eng', 'fra', 'eng', 'spa', 'deu', 'en,fr,de', 'en', 'ita', 'jpn']\n",
      "Prédictions Naive Bayes       : ['German', 'English', 'French', 'English', 'Spanish', 'German', 'Galician', 'English', 'Italian', 'Japanese']\n"
     ]
    }
   ],
   "source": [
    "# Affichage des prédictions\n",
    "print('Langue réelle                 :',lang_exemples)\n",
    "print('Prédictions Naive Bayes       :',lang_id_nb(exemples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8d1c1-e5dd-4a17-8aee-1305fbf4c45a",
   "metadata": {},
   "source": [
    "> **Recherche des phrases mal classées par Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d458174a-c2e1-41e8-a84a-35584eaac751",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN°Ligne\tL. réelle\tPréd. Naive B.\t\tPhrase\n",
      "1 \t 0 \t- Berber langu \t\tKabyle       \t\tYeqber uɛebbuḍ-iw seg wayen ččiɣ.  (proba=0.85)\n",
      "2 \t 5 \t- Hindi \t\tMarathi      \t\tबॅटर आऊट हो गया था।  (proba=0.85)\n",
      "3 \t 14 \t- Turkish \t\tKabyle       \t\tHadi yavrum kemik.  (proba=0.61)\n",
      "4 \t 33 \t- Serbian \t\tCroatian     \t\tMomci su predvidljivi.  (proba=0.68)\n",
      "5 \t 49 \t- Swabian \t\tGerman       \t\tDie Fraind hoißet sich aufrichdig, die Faind send's.  (proba=1.00)\n",
      "6 \t 55 \t- Slovak \t\tHungarian    \t\tHluk mi vadí.  (proba=0.90)\n",
      "7 \t 75 \t- Berber langu \t\tKabyle       \t\tBɣiɣ ad lemdeɣ ad cnuɣ am kemmini.  (proba=0.84)\n",
      "8 \t 97 \t- Norwegian Ny \t\tNorwegian Bo \t\tKvar er utgangen?  (proba=0.56)\n",
      "9 \t 111 \t- Kabyle \t\tBerber langu \t\tTom ad iṛuḥ ɣer Ustṛalya.  (proba=0.56)\n",
      "10 \t 138 \t- Crimean Tata \t\tBerber langu \t\tÖtmek taze degil.  (proba=0.86)\n",
      "11 \t 140 \t- Ukrainian \t\tRussian      \t\tЯ правий?  (proba=0.54)\n",
      "12 \t 184 \t- Spanish \t\tPortuguese   \t\tMalos pastores, malas ovejas.  (proba=0.56)\n",
      "13 \t 192 \t- Slovenian \t\tCroatian     \t\tMoje ime je Sally.  (proba=0.87)\n",
      "14 \t 250 \t- Berber langu \t\tKabyle       \t\tTelliḍ tettiniḍ-d tidet.  (proba=0.65)\n",
      "15 \t 251 \t- Kabyle \t\tBerber langu \t\tTapurtugit d tutlayt taṛumant.  (proba=0.53)\n",
      "16 \t 263 \t- Kabyle \t\tBerber langu \t\tYenna-d Tom d akken yugi ad yessuter i Mary ad texdem ayagi.  (proba=1.00)\n",
      "17 \t 269 \t- Zaza \t\tNorthern Kur \t\tMi hetkarîya may û pîyê xo kerd.  (proba=1.00)\n",
      "18 \t 315 \t- Kabyle \t\tBerber langu \t\tBeṛka i terram iman-nwen d imehbal.  (proba=0.61)\n",
      "19 \t 341 \t- Berber langu \t\tKabyle       \t\tTtetten kullec.  (proba=0.94)\n",
      "20 \t 406 \t- Russian \t\tMacedonian   \t\tПицца готова.  (proba=0.58)\n",
      "21 \t 410 \t- Eastern Mari \t\tRussian      \t\tЛек мыланем марлан.  (proba=0.95)\n",
      "22 \t 456 \t- Portuguese \t\tTurkish      \t\tSabem?  (proba=0.45)\n",
      "23 \t 487 \t- Kabyle \t\tBerber langu \t\tUr ttun ara ansi i d-frurin.  (proba=0.68)\n",
      "24 \t 488 \t- Kabyle \t\tBerber langu \t\tIwacu ur tensimireḍ ara Tom?  (proba=0.52)\n",
      "25 \t 492 \t- Ottoman Turk \t\tIranian Pers \t\tبو یڭی می؟  (proba=0.97)\n",
      "26 \t 523 \t- Standard Mor \t\tYue Chinese  \t\tⵜⴳⴰ ⴱⴰⵀⵔⴰ ⴽⴰⵜⵉ ⵜⵉⵎⵉⵖⵉⵙⵜ ⴼ ⵇⵇⴰⵃ ⵏ ⵉⵎⵣⵔⴰⵡⵏ ⴳ ⵜⵎⵖⵔⵉⵜ ⵏⵏⵖ.  (proba=1.00)\n",
      "27 \t 559 \t- Kabyle \t\tBerber langu \t\tYettas-d yal ass deg usrag-a.  (proba=1.00)\n",
      "28 \t 578 \t- Kabyle \t\tBerber langu \t\tMa tufrareḍ-d amzun d nekk i d-yufraren.  (proba=1.00)\n",
      "29 \t 597 \t- Kabyle \t\tBerber langu \t\tMelmi i tent-tɣunza?  (proba=0.92)\n",
      "30 \t 606 \t- Kabyle \t\tBerber langu \t\tAs-d ar wexxam.  (proba=0.81)\n"
     ]
    }
   ],
   "source": [
    "n_bad_max = 30\n",
    "n_bad = 0\n",
    "print(\"\\tN°Ligne\\tL. réelle\\tPréd. Naive B.\\t\\tPhrase\")\n",
    "for i in range(len(df)):\n",
    "    if (n_bad<n_bad_max):\n",
    "        if (lan_to_language[df['lan_code'].iloc[i]] != lang_id_nb(df['sentence'].iloc[i])):\n",
    "            n_bad +=1\n",
    "            print(n_bad,'\\t',i,'\\t-',lan_to_language[df['lan_code'].iloc[i]][:12],'\\t\\t'+lang_id_nb(df['sentence'].iloc[i]).ljust(12)[:12],'\\t\\t'+\n",
    "                  df['sentence'].iloc[i],\" (proba={:.2f}\".format(max(clf_nb.predict_proba(create_BOW([df['sentence'].iloc[i]]))[0]))+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7acb8eb0-5654-4f6b-a82b-0df2c2686eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no 510000  -  9040.251601696014 s ( 0.027617145376077665 s/id )           \n",
      "Nombre de phrases prises en compte : 329683\n",
      "Durée de traitement : 9098.671464204788 secondes, c'est à dire  0.027598242748958206 s/identification\n",
      "Accuracy de xlm-roberta : 0.9775056645322932\n",
      "Accuracy de Naïve Bayes : 0.9932419930660665\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "y_ext_predicted = []\n",
    "y_ext_actual = []\n",
    "y_nb_predicted=[]\n",
    "dict_xlmr  = {\"ar\":\"ara\", \"bg\":\"bul\", \"de\":\"deu\", \"el\": \"ell\", \"en\":\"eng\", \"es\":\"spa\", \"fr\":\"fra\", \"hi\": \"hin\",\"it\":\"ita\",\"ja\":\"jpn\", \\\n",
    "              \"nl\":\"nld\", \"pl\":\"pol\", \"pt\":\"por\", \"ru\":\"rus\", \"sw\":\"swh\", \"th\":\"tha\", \"tr\":\"tur\", \"ur\": \"urd\", \"vi\":\"vie\", \"zh\":\"cmn\"}\n",
    "lang_available = list(dict_xlmr.values())\n",
    "lang_id_model_ext = pipeline('text-classification',model=\"papluca/xlm-roberta-base-language-detection\")\n",
    "start_time = time.time()\n",
    "j= 0\n",
    "for i in range(len(df_test)):\n",
    "    if df_test.lan_code.iloc[i] in lang_available:\n",
    "        y_ext_predicted.append(dict_xlmr[lang_id_model_ext(df_test.sentence.iloc[i][:1000])[0]['label']])\n",
    "        y_ext_actual.append(df_test.lan_code.iloc[i])\n",
    "        y_nb_predicted.append(y_pred[i])\n",
    "        if (i-j)>=10000:\n",
    "            j = (i//10000)*10000\n",
    "            d = (time.time()- start_time)\n",
    "            print(\"no\",j,\" - \",d,\"s (\",d/len(y_ext_predicted),\"s/id )          \",end=\"\\r\")\n",
    "            \n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(\"\")\n",
    "print(\"Nombre de phrases prises en compte :\",len(y_ext_predicted))\n",
    "print(\"Durée de traitement :\",duration,\"secondes, c'est à dire \", duration/len(y_ext_predicted),\"s/identification\")\n",
    "print(\"Accuracy de xlm-roberta :\",accuracy_score(y_ext_actual, y_ext_predicted))\n",
    "print(\"Accuracy de Naïve Bayes :\",accuracy_score(y_ext_actual, y_nb_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
